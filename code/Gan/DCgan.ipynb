{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6292a53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이전의 gan과 달라진점 중간에 barch nomarize사용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d581d953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules import loss\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1123179",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channel_img, features_d):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            # N x features_d x 32 x 32\n",
    "            #input output \n",
    "        nn.Conv2d(channel_img,features_d,kernel_size=4,stride=2,padding=1),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        self._block(features_d, features_d * 2, 4, 2, 1),\n",
    "        self._block(features_d*2 , features_d * 4, 4, 2, 1),\n",
    "        self._block(features_d*4 , features_d * 8, 4, 2, 1),\n",
    "            \n",
    "        nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0), \n",
    "          nn.Sigmoid(),   \n",
    "        )\n",
    "        \n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "        nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            stride,\n",
    "            padding,\n",
    "            bias = False,\n",
    "        ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.disc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "117b22b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# channels_img : 64 *64\n",
    "# features_d : 첫 layer에서 나오는 features의 dim\n",
    "# output 마지막은 sigmoid를 사용해야하기 때문에 \n",
    "# n * features_d *8*1*1d의 크기로 만든다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19269085",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, channels_noise, channels_img, features_g):\n",
    "        super(Generator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "        self._block(channels_noise, features_g *16,4,1,0),\n",
    "        self._block(features_g * 16, features_g * 8, 4, 2, 1), # img: 8 x 8\n",
    "        self._block(features_g * 8, features_g * 4, 4, 2, 1), # img: 16 x 16\n",
    "        self._block(features_g * 4, features_g * 2, 4, 2, 1), # img: 32 x 32    \n",
    "        \n",
    "         nn.ConvTranspose2d(\n",
    "          features_g * 2, channels_img, kernel_size=4, stride=2, padding=1\n",
    "        ),\n",
    "        # Output: N x channels_img x 64 x 64\n",
    "        nn.Tanh(),\n",
    "      )\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "        nn.ConvTranspose2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride,\n",
    "            padding,\n",
    "            bias = False,\n",
    "        ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad040929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.ConvTranspose2d 형상을 늘려줌 업스케일링 과정 \n",
    "# output N *channels_img * 64 *64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80cab5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "  # Initializes weights according to the DCGAN paper\n",
    "  for m in model.modules():\n",
    "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "      nn.init.normal_(m.weight.data, 0.0, 0.02) # mean 0, std 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04a2710f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LEARNING_RATE = 2e-4 # could also use two lrs, one for gen and one for disc (0.001)\n",
    "BATCH_SIZE = 128\n",
    "IMAGE_SIZE = 64\n",
    "CHANNELS_IMG = 1\n",
    "NOISE_DIM = 50\n",
    "NUM_EPOCHS = 10\n",
    "FEATURES_DISC = 64\n",
    "FEATURES_GEN = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4770240",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = transforms.Compose(\n",
    "  [\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "      [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)]\n",
    "    ),\n",
    "  ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e49b40a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.MNIST(root=\"./Desktop\", train=True, transform=transforms,\n",
    "                       download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac342a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96e58272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (disc): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(2, 2), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(2, 2), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (5): Conv2d(512, 1, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (6): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "gen = Generator(NOISE_DIM, CHANNELS_IMG, FEATURES_GEN).to(device)\n",
    "disc = Discriminator(CHANNELS_IMG, FEATURES_DISC).to(device)\n",
    "initialize_weights(gen)\n",
    "initialize_weights(disc)\n",
    "\n",
    "# in the paper, it is stable when B1 is 0.5 than 0.9\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))  \n",
    "opt_disc = optim.Adam(disc.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "fixed_noise = torch.randn(32, NOISE_DIM, 1, 1).to(device)\n",
    "writer_real = SummaryWriter(f\"./runs/DCGAN_MNIST/DC_real\")\n",
    "writer_fake = SummaryWriter(f\"./runs/DCGAN_MNIST/DC_fake\")\n",
    "step = 0\n",
    "\n",
    "# BN, Dropout Train mode\n",
    "gen.train()\n",
    "disc.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35476eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/10] Batch 0/469           Loss D : 0.6952, loss G: 0.6983\n",
      "Epoch [0/10] Batch 100/469           Loss D : 0.2223, loss G: 1.4774\n",
      "Epoch [0/10] Batch 200/469           Loss D : 0.0532, loss G: 2.9184\n",
      "Epoch [0/10] Batch 300/469           Loss D : 0.0335, loss G: 3.5027\n",
      "Epoch [0/10] Batch 400/469           Loss D : 0.7622, loss G: 1.5555\n",
      "Epoch [1/10] Batch 0/469           Loss D : 0.6399, loss G: 0.8227\n",
      "Epoch [1/10] Batch 100/469           Loss D : 0.6521, loss G: 0.8574\n",
      "Epoch [1/10] Batch 200/469           Loss D : 0.6677, loss G: 0.7123\n",
      "Epoch [1/10] Batch 300/469           Loss D : 0.6845, loss G: 0.7408\n",
      "Epoch [1/10] Batch 400/469           Loss D : 0.6594, loss G: 0.7385\n",
      "Epoch [2/10] Batch 0/469           Loss D : 0.7031, loss G: 1.0660\n",
      "Epoch [2/10] Batch 100/469           Loss D : 0.6619, loss G: 0.7637\n",
      "Epoch [2/10] Batch 200/469           Loss D : 0.6678, loss G: 0.6997\n",
      "Epoch [2/10] Batch 300/469           Loss D : 0.6550, loss G: 0.8313\n",
      "Epoch [2/10] Batch 400/469           Loss D : 0.6949, loss G: 0.7157\n",
      "Epoch [3/10] Batch 0/469           Loss D : 0.6916, loss G: 0.7026\n",
      "Epoch [3/10] Batch 100/469           Loss D : 0.6788, loss G: 0.7157\n",
      "Epoch [3/10] Batch 200/469           Loss D : 0.6779, loss G: 0.6497\n",
      "Epoch [3/10] Batch 300/469           Loss D : 0.6609, loss G: 0.8575\n",
      "Epoch [3/10] Batch 400/469           Loss D : 0.6352, loss G: 0.8093\n",
      "Epoch [4/10] Batch 0/469           Loss D : 0.6375, loss G: 0.7653\n",
      "Epoch [4/10] Batch 100/469           Loss D : 0.6510, loss G: 0.8166\n",
      "Epoch [4/10] Batch 200/469           Loss D : 0.7021, loss G: 0.6989\n",
      "Epoch [4/10] Batch 300/469           Loss D : 0.6822, loss G: 0.7272\n",
      "Epoch [4/10] Batch 400/469           Loss D : 0.6513, loss G: 0.7851\n",
      "Epoch [5/10] Batch 0/469           Loss D : 0.6479, loss G: 0.6099\n",
      "Epoch [5/10] Batch 100/469           Loss D : 0.6499, loss G: 1.0436\n",
      "Epoch [5/10] Batch 200/469           Loss D : 0.6317, loss G: 0.8232\n",
      "Epoch [5/10] Batch 300/469           Loss D : 0.6355, loss G: 0.8401\n",
      "Epoch [5/10] Batch 400/469           Loss D : 0.6183, loss G: 0.8109\n",
      "Epoch [6/10] Batch 0/469           Loss D : 0.6253, loss G: 0.9737\n",
      "Epoch [6/10] Batch 100/469           Loss D : 0.6241, loss G: 0.8477\n",
      "Epoch [6/10] Batch 200/469           Loss D : 0.9191, loss G: 0.8546\n",
      "Epoch [6/10] Batch 300/469           Loss D : 0.6045, loss G: 1.1132\n",
      "Epoch [6/10] Batch 400/469           Loss D : 0.6189, loss G: 0.8769\n",
      "Epoch [7/10] Batch 0/469           Loss D : 0.6933, loss G: 0.8768\n",
      "Epoch [7/10] Batch 100/469           Loss D : 0.6173, loss G: 1.1682\n",
      "Epoch [7/10] Batch 200/469           Loss D : 0.6674, loss G: 0.6815\n",
      "Epoch [7/10] Batch 300/469           Loss D : 0.5778, loss G: 0.9339\n",
      "Epoch [7/10] Batch 400/469           Loss D : 0.6135, loss G: 0.8081\n",
      "Epoch [8/10] Batch 0/469           Loss D : 0.5678, loss G: 0.9012\n",
      "Epoch [8/10] Batch 100/469           Loss D : 0.6091, loss G: 0.7797\n",
      "Epoch [8/10] Batch 200/469           Loss D : 0.5767, loss G: 1.2672\n",
      "Epoch [8/10] Batch 300/469           Loss D : 0.5848, loss G: 0.7855\n",
      "Epoch [8/10] Batch 400/469           Loss D : 0.5502, loss G: 1.1745\n",
      "Epoch [9/10] Batch 0/469           Loss D : 0.6326, loss G: 0.6567\n",
      "Epoch [9/10] Batch 100/469           Loss D : 0.5043, loss G: 1.2057\n",
      "Epoch [9/10] Batch 200/469           Loss D : 0.5199, loss G: 1.1734\n",
      "Epoch [9/10] Batch 300/469           Loss D : 0.5036, loss G: 0.9755\n",
      "Epoch [9/10] Batch 400/469           Loss D : 0.5734, loss G: 1.1687\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "  # Target labels not needed! ~~ unsupervised\n",
    "  for batch_idx, (real, _) in enumerate(dataloader):\n",
    "    real = real.to(device)\n",
    "    noise = torch.randn(BATCH_SIZE, NOISE_DIM, 1, 1).to(device)\n",
    "    fake = gen(noise)\n",
    "\n",
    "    \n",
    "    ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "    disc_real = disc(real).reshape(-1)\n",
    "    lossD_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "  \n",
    "    disc_fake = disc(fake.detach()).reshape(-1)\n",
    "    lossD_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "    lossD = (lossD_real + lossD_fake) / 2\n",
    "    disc.zero_grad()\n",
    "    lossD.backward()\n",
    "    opt_disc.step()\n",
    "\n",
    "    ### Train Generator: min log(1 - D(G(z)) <-> max log(D(G(z))\n",
    "\n",
    "    output = disc(fake).reshape(-1)\n",
    "    lossG = criterion(output, torch.ones_like(output))\n",
    "    gen.zero_grad()\n",
    "    lossG.backward()\n",
    "    opt_gen.step()\n",
    "\n",
    "    # Print losses occasionally and print to tensorboard\n",
    "    if batch_idx % 100 == 0:\n",
    "      print(\n",
    "        f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(dataloader)} \\\n",
    "          Loss D : {lossD:.4f}, loss G: {lossG:.4f}\"\n",
    "      )\n",
    "\n",
    "      with torch.no_grad():\n",
    "              fake = gen(fixed_noise)\n",
    "              # take out (up to) 32 examples\n",
    "              img_grid_real = torchvision.utils.make_grid(\n",
    "                  real[:32], normalize=True\n",
    "              )\n",
    "              img_grid_fake = torchvision.utils.make_grid(\n",
    "                  fake[:32], normalize=True\n",
    "              )\n",
    "\n",
    "              writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
    "              writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
    "\n",
    "      step += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99083076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 64, 64])\n",
      "torch.Size([128, 1, 64, 64])\n",
      "torch.Size([128, 1, 64, 64])\n",
      "torch.Size([128, 1, 64, 64])\n",
      "torch.Size([128, 1, 64, 64])\n",
      "torch.Size([128, 1, 64, 64])\n",
      "torch.Size([128, 1, 64, 64])\n",
      "torch.Size([128, 1, 64, 64])\n",
      "torch.Size([128, 1, 64, 64])\n",
      "torch.Size([128, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "  # Target labels not needed! ~~ unsupervised\n",
    "  for batch_idx, (real, _) in enumerate(dataloader):\n",
    "    real = real.to(device)\n",
    "    noise = torch.randn(BATCH_SIZE, NOISE_DIM, 1, 1).to(device)\n",
    "    fake = gen(noise)\n",
    "    print(real.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce070d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.7_imagetuto",
   "language": "python",
   "name": "3.7_imagetuto"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d25be9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules import loss\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5da5f3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real과 Fake를 구분하는 Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            nn.Linear(in_features, 128),\n",
    "            nn.LeakyReLU(0,1),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    def forward(self ,x ):\n",
    "        return self.disc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8a92277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 생성모델 \n",
    "# z_dim이라는 노이즈를 받고 output_size로 이미지 사이즈를 받음 여기선 28x28x1\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, img_dim):\n",
    "        super().__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            nn.Linear(z_dim,256),\n",
    "            nn.LeakyReLU(0,1),\n",
    "            nn.Linear(256, img_dim),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "    def forward(self ,x):\n",
    "        return self.gen(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7b23ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "lr = 3e-4\n",
    "z_dim = 10 # 128, 256\n",
    "image_dim = 28 * 28 * 1 # 784\n",
    "batch_size = 32\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d79959a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "disc = Discriminator(image_dim).to(device)\n",
    "gen = Generator(z_dim, image_dim).to(device)\n",
    "fixed_noise = torch.randn((batch_size, z_dim)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca157fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = transforms.Compose(\n",
    "  [transforms.ToTensor(),\n",
    "  transforms.Normalize((0.5,), (0.5,)),]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "abb37715",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.MNIST(root='./Desktop', transform=transforms, download=True)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "opt_disc = optim.Adam(disc.parameters(), lr=lr)\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=lr)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c12aba09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x14e8e6bd6c8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchvision.transforms.functional import to_pil_image\n",
    "import matplotlib.pylab as plt\n",
    "img, label = dataset[0]\n",
    "plt.imshow(to_pil_image(img),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "699d8c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_fake = SummaryWriter(f\"runs/GAN_MNIST/fake\")\n",
    "writer_real= SummaryWriter(f\"runs/GAN_MNIST/real\")\n",
    "\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b99b1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50] Batch 0/1875                       Loss D: 0.7414, loss G: 0.6619\n",
      "Epoch [1/50] Batch 0/1875                       Loss D: 0.4758, loss G: 1.0929\n",
      "Epoch [2/50] Batch 0/1875                       Loss D: 0.2837, loss G: 1.5138\n",
      "Epoch [3/50] Batch 0/1875                       Loss D: 0.2076, loss G: 1.8483\n",
      "Epoch [4/50] Batch 0/1875                       Loss D: 0.6448, loss G: 0.9960\n",
      "Epoch [5/50] Batch 0/1875                       Loss D: 0.3977, loss G: 1.1637\n",
      "Epoch [6/50] Batch 0/1875                       Loss D: 0.5750, loss G: 1.2245\n",
      "Epoch [7/50] Batch 0/1875                       Loss D: 0.7785, loss G: 0.8678\n",
      "Epoch [8/50] Batch 0/1875                       Loss D: 0.5723, loss G: 0.9407\n",
      "Epoch [9/50] Batch 0/1875                       Loss D: 0.7452, loss G: 0.7082\n",
      "Epoch [10/50] Batch 0/1875                       Loss D: 0.8858, loss G: 0.9244\n",
      "Epoch [11/50] Batch 0/1875                       Loss D: 0.6992, loss G: 0.7453\n",
      "Epoch [12/50] Batch 0/1875                       Loss D: 0.7071, loss G: 1.0389\n",
      "Epoch [13/50] Batch 0/1875                       Loss D: 0.9668, loss G: 0.5936\n",
      "Epoch [14/50] Batch 0/1875                       Loss D: 0.3232, loss G: 2.2689\n",
      "Epoch [15/50] Batch 0/1875                       Loss D: 0.6798, loss G: 0.9812\n",
      "Epoch [16/50] Batch 0/1875                       Loss D: 0.5815, loss G: 1.3064\n",
      "Epoch [17/50] Batch 0/1875                       Loss D: 0.7668, loss G: 0.7897\n",
      "Epoch [18/50] Batch 0/1875                       Loss D: 0.6418, loss G: 1.1140\n",
      "Epoch [19/50] Batch 0/1875                       Loss D: 0.6244, loss G: 0.9521\n",
      "Epoch [20/50] Batch 0/1875                       Loss D: 0.5888, loss G: 1.1499\n",
      "Epoch [21/50] Batch 0/1875                       Loss D: 0.7115, loss G: 0.9818\n",
      "Epoch [22/50] Batch 0/1875                       Loss D: 0.4852, loss G: 1.5213\n",
      "Epoch [23/50] Batch 0/1875                       Loss D: 0.6373, loss G: 0.9509\n",
      "Epoch [24/50] Batch 0/1875                       Loss D: 0.7056, loss G: 1.0240\n",
      "Epoch [25/50] Batch 0/1875                       Loss D: 0.5650, loss G: 1.0872\n",
      "Epoch [26/50] Batch 0/1875                       Loss D: 0.8865, loss G: 0.7993\n",
      "Epoch [27/50] Batch 0/1875                       Loss D: 0.5491, loss G: 0.8534\n",
      "Epoch [28/50] Batch 0/1875                       Loss D: 0.6068, loss G: 1.1549\n",
      "Epoch [29/50] Batch 0/1875                       Loss D: 0.6468, loss G: 1.1266\n",
      "Epoch [30/50] Batch 0/1875                       Loss D: 0.7402, loss G: 1.0106\n",
      "Epoch [31/50] Batch 0/1875                       Loss D: 0.4775, loss G: 1.2911\n",
      "Epoch [32/50] Batch 0/1875                       Loss D: 0.6531, loss G: 1.0241\n",
      "Epoch [33/50] Batch 0/1875                       Loss D: 0.8243, loss G: 0.8198\n",
      "Epoch [34/50] Batch 0/1875                       Loss D: 0.7135, loss G: 0.7191\n",
      "Epoch [35/50] Batch 0/1875                       Loss D: 0.6136, loss G: 1.0947\n",
      "Epoch [36/50] Batch 0/1875                       Loss D: 0.5538, loss G: 1.0219\n",
      "Epoch [37/50] Batch 0/1875                       Loss D: 0.7503, loss G: 0.8492\n",
      "Epoch [38/50] Batch 0/1875                       Loss D: 0.5332, loss G: 1.2317\n",
      "Epoch [39/50] Batch 0/1875                       Loss D: 0.6462, loss G: 0.9008\n",
      "Epoch [40/50] Batch 0/1875                       Loss D: 0.5029, loss G: 1.1199\n",
      "Epoch [41/50] Batch 0/1875                       Loss D: 0.6279, loss G: 1.0830\n",
      "Epoch [42/50] Batch 0/1875                       Loss D: 0.6208, loss G: 1.2732\n",
      "Epoch [43/50] Batch 0/1875                       Loss D: 0.4810, loss G: 1.1023\n",
      "Epoch [44/50] Batch 0/1875                       Loss D: 0.5936, loss G: 1.0926\n",
      "Epoch [45/50] Batch 0/1875                       Loss D: 0.6205, loss G: 1.1592\n",
      "Epoch [46/50] Batch 0/1875                       Loss D: 0.4694, loss G: 1.1468\n",
      "Epoch [47/50] Batch 0/1875                       Loss D: 0.7269, loss G: 0.8095\n",
      "Epoch [48/50] Batch 0/1875                       Loss D: 0.5574, loss G: 1.0519\n",
      "Epoch [49/50] Batch 0/1875                       Loss D: 0.5480, loss G: 1.1338\n",
      "Epoch [50/50] Batch 0/1875                       Loss D: 0.7143, loss G: 0.8810\n",
      "Epoch [51/50] Batch 0/1875                       Loss D: 0.6451, loss G: 0.7973\n",
      "Epoch [52/50] Batch 0/1875                       Loss D: 0.5330, loss G: 0.9940\n",
      "Epoch [53/50] Batch 0/1875                       Loss D: 0.6967, loss G: 0.9333\n",
      "Epoch [54/50] Batch 0/1875                       Loss D: 0.6491, loss G: 0.9186\n",
      "Epoch [55/50] Batch 0/1875                       Loss D: 0.5616, loss G: 1.1408\n",
      "Epoch [56/50] Batch 0/1875                       Loss D: 0.6340, loss G: 0.7438\n",
      "Epoch [57/50] Batch 0/1875                       Loss D: 0.5167, loss G: 1.1586\n",
      "Epoch [58/50] Batch 0/1875                       Loss D: 0.5899, loss G: 1.0389\n",
      "Epoch [59/50] Batch 0/1875                       Loss D: 0.7262, loss G: 0.8318\n",
      "Epoch [60/50] Batch 0/1875                       Loss D: 0.5957, loss G: 1.0487\n",
      "Epoch [61/50] Batch 0/1875                       Loss D: 0.6328, loss G: 0.8536\n",
      "Epoch [62/50] Batch 0/1875                       Loss D: 0.6396, loss G: 0.8461\n",
      "Epoch [63/50] Batch 0/1875                       Loss D: 0.6909, loss G: 0.8511\n",
      "Epoch [64/50] Batch 0/1875                       Loss D: 0.6333, loss G: 0.8981\n",
      "Epoch [65/50] Batch 0/1875                       Loss D: 0.6326, loss G: 0.9397\n",
      "Epoch [66/50] Batch 0/1875                       Loss D: 0.6326, loss G: 0.8482\n",
      "Epoch [67/50] Batch 0/1875                       Loss D: 0.8280, loss G: 0.6778\n",
      "Epoch [68/50] Batch 0/1875                       Loss D: 0.7149, loss G: 0.8278\n",
      "Epoch [69/50] Batch 0/1875                       Loss D: 0.6341, loss G: 0.9785\n",
      "Epoch [70/50] Batch 0/1875                       Loss D: 0.6556, loss G: 0.9210\n",
      "Epoch [71/50] Batch 0/1875                       Loss D: 0.5936, loss G: 0.8785\n",
      "Epoch [72/50] Batch 0/1875                       Loss D: 0.6970, loss G: 0.8423\n",
      "Epoch [73/50] Batch 0/1875                       Loss D: 0.6232, loss G: 0.8792\n",
      "Epoch [74/50] Batch 0/1875                       Loss D: 0.6410, loss G: 0.8979\n",
      "Epoch [75/50] Batch 0/1875                       Loss D: 0.6501, loss G: 0.9261\n",
      "Epoch [76/50] Batch 0/1875                       Loss D: 0.6569, loss G: 0.8234\n",
      "Epoch [77/50] Batch 0/1875                       Loss D: 0.6092, loss G: 0.9503\n",
      "Epoch [78/50] Batch 0/1875                       Loss D: 0.7357, loss G: 0.8217\n",
      "Epoch [79/50] Batch 0/1875                       Loss D: 0.6786, loss G: 0.8514\n",
      "Epoch [80/50] Batch 0/1875                       Loss D: 0.6953, loss G: 0.8052\n",
      "Epoch [81/50] Batch 0/1875                       Loss D: 0.7029, loss G: 0.8085\n",
      "Epoch [82/50] Batch 0/1875                       Loss D: 0.6837, loss G: 0.9312\n",
      "Epoch [83/50] Batch 0/1875                       Loss D: 0.7074, loss G: 0.6694\n",
      "Epoch [84/50] Batch 0/1875                       Loss D: 0.7280, loss G: 0.7177\n",
      "Epoch [85/50] Batch 0/1875                       Loss D: 0.6651, loss G: 0.8077\n",
      "Epoch [86/50] Batch 0/1875                       Loss D: 0.6423, loss G: 0.8342\n",
      "Epoch [87/50] Batch 0/1875                       Loss D: 0.6536, loss G: 0.9016\n",
      "Epoch [88/50] Batch 0/1875                       Loss D: 0.6071, loss G: 0.8539\n",
      "Epoch [89/50] Batch 0/1875                       Loss D: 0.7478, loss G: 0.7417\n",
      "Epoch [90/50] Batch 0/1875                       Loss D: 0.6970, loss G: 0.6791\n",
      "Epoch [91/50] Batch 0/1875                       Loss D: 0.7419, loss G: 0.6795\n",
      "Epoch [92/50] Batch 0/1875                       Loss D: 0.6453, loss G: 0.8934\n",
      "Epoch [93/50] Batch 0/1875                       Loss D: 0.7475, loss G: 0.6645\n",
      "Epoch [94/50] Batch 0/1875                       Loss D: 0.6266, loss G: 0.8776\n",
      "Epoch [95/50] Batch 0/1875                       Loss D: 0.6291, loss G: 0.7561\n",
      "Epoch [96/50] Batch 0/1875                       Loss D: 0.6984, loss G: 0.6978\n",
      "Epoch [97/50] Batch 0/1875                       Loss D: 0.7836, loss G: 0.7113\n",
      "Epoch [98/50] Batch 0/1875                       Loss D: 0.7438, loss G: 0.6738\n",
      "Epoch [99/50] Batch 0/1875                       Loss D: 0.6233, loss G: 0.8584\n",
      "Epoch [100/50] Batch 0/1875                       Loss D: 0.6293, loss G: 0.8308\n",
      "Epoch [101/50] Batch 0/1875                       Loss D: 0.7071, loss G: 0.7280\n",
      "Epoch [102/50] Batch 0/1875                       Loss D: 0.7305, loss G: 0.6935\n",
      "Epoch [103/50] Batch 0/1875                       Loss D: 0.7832, loss G: 0.6470\n",
      "Epoch [104/50] Batch 0/1875                       Loss D: 0.6835, loss G: 0.8363\n",
      "Epoch [105/50] Batch 0/1875                       Loss D: 0.6563, loss G: 0.7809\n",
      "Epoch [106/50] Batch 0/1875                       Loss D: 0.7201, loss G: 0.7505\n",
      "Epoch [107/50] Batch 0/1875                       Loss D: 0.6432, loss G: 0.7848\n",
      "Epoch [108/50] Batch 0/1875                       Loss D: 0.6448, loss G: 0.8021\n",
      "Epoch [109/50] Batch 0/1875                       Loss D: 0.6089, loss G: 0.8826\n",
      "Epoch [110/50] Batch 0/1875                       Loss D: 0.6584, loss G: 0.7204\n",
      "Epoch [111/50] Batch 0/1875                       Loss D: 0.6404, loss G: 0.8083\n",
      "Epoch [112/50] Batch 0/1875                       Loss D: 0.6525, loss G: 0.7597\n",
      "Epoch [113/50] Batch 0/1875                       Loss D: 0.6265, loss G: 0.7802\n",
      "Epoch [114/50] Batch 0/1875                       Loss D: 0.7657, loss G: 0.7108\n",
      "Epoch [115/50] Batch 0/1875                       Loss D: 0.6746, loss G: 0.8232\n",
      "Epoch [116/50] Batch 0/1875                       Loss D: 0.6044, loss G: 0.9760\n",
      "Epoch [117/50] Batch 0/1875                       Loss D: 0.7040, loss G: 0.7688\n",
      "Epoch [118/50] Batch 0/1875                       Loss D: 0.7448, loss G: 0.7586\n",
      "Epoch [119/50] Batch 0/1875                       Loss D: 0.6762, loss G: 0.7932\n",
      "Epoch [120/50] Batch 0/1875                       Loss D: 0.7295, loss G: 0.7613\n",
      "Epoch [121/50] Batch 0/1875                       Loss D: 0.7082, loss G: 0.7689\n",
      "Epoch [122/50] Batch 0/1875                       Loss D: 0.6697, loss G: 0.8276\n",
      "Epoch [123/50] Batch 0/1875                       Loss D: 0.6966, loss G: 0.8153\n",
      "Epoch [124/50] Batch 0/1875                       Loss D: 0.7493, loss G: 0.7016\n",
      "Epoch [125/50] Batch 0/1875                       Loss D: 0.7103, loss G: 0.6801\n",
      "Epoch [126/50] Batch 0/1875                       Loss D: 0.7083, loss G: 0.8402\n",
      "Epoch [127/50] Batch 0/1875                       Loss D: 0.7248, loss G: 0.7072\n",
      "Epoch [128/50] Batch 0/1875                       Loss D: 0.6450, loss G: 0.8699\n",
      "Epoch [129/50] Batch 0/1875                       Loss D: 0.6263, loss G: 0.9042\n",
      "Epoch [130/50] Batch 0/1875                       Loss D: 0.6620, loss G: 0.8342\n",
      "Epoch [131/50] Batch 0/1875                       Loss D: 0.7168, loss G: 0.8114\n",
      "Epoch [132/50] Batch 0/1875                       Loss D: 0.6079, loss G: 0.8980\n",
      "Epoch [133/50] Batch 0/1875                       Loss D: 0.7545, loss G: 0.7050\n",
      "Epoch [134/50] Batch 0/1875                       Loss D: 0.6144, loss G: 0.7459\n",
      "Epoch [135/50] Batch 0/1875                       Loss D: 0.6015, loss G: 0.8444\n",
      "Epoch [136/50] Batch 0/1875                       Loss D: 0.5828, loss G: 0.9809\n",
      "Epoch [137/50] Batch 0/1875                       Loss D: 0.7822, loss G: 0.6556\n",
      "Epoch [138/50] Batch 0/1875                       Loss D: 0.7255, loss G: 0.7123\n",
      "Epoch [139/50] Batch 0/1875                       Loss D: 0.7842, loss G: 0.6452\n",
      "Epoch [140/50] Batch 0/1875                       Loss D: 0.5919, loss G: 0.8524\n",
      "Epoch [141/50] Batch 0/1875                       Loss D: 0.7277, loss G: 0.7160\n",
      "Epoch [142/50] Batch 0/1875                       Loss D: 0.7129, loss G: 0.7872\n",
      "Epoch [143/50] Batch 0/1875                       Loss D: 0.6959, loss G: 0.7494\n",
      "Epoch [144/50] Batch 0/1875                       Loss D: 0.6951, loss G: 0.7643\n",
      "Epoch [145/50] Batch 0/1875                       Loss D: 0.8045, loss G: 0.6572\n",
      "Epoch [146/50] Batch 0/1875                       Loss D: 0.6827, loss G: 0.7517\n",
      "Epoch [147/50] Batch 0/1875                       Loss D: 0.6962, loss G: 0.7210\n",
      "Epoch [148/50] Batch 0/1875                       Loss D: 0.7829, loss G: 0.7546\n",
      "Epoch [149/50] Batch 0/1875                       Loss D: 0.6555, loss G: 0.8033\n",
      "Epoch [150/50] Batch 0/1875                       Loss D: 0.6955, loss G: 0.7460\n",
      "Epoch [151/50] Batch 0/1875                       Loss D: 0.7518, loss G: 0.6304\n",
      "Epoch [152/50] Batch 0/1875                       Loss D: 0.6789, loss G: 0.7152\n",
      "Epoch [153/50] Batch 0/1875                       Loss D: 0.7308, loss G: 0.6555\n",
      "Epoch [154/50] Batch 0/1875                       Loss D: 0.7133, loss G: 0.7501\n",
      "Epoch [155/50] Batch 0/1875                       Loss D: 0.7063, loss G: 0.7288\n",
      "Epoch [156/50] Batch 0/1875                       Loss D: 0.7017, loss G: 0.7336\n",
      "Epoch [157/50] Batch 0/1875                       Loss D: 0.6851, loss G: 0.7535\n",
      "Epoch [158/50] Batch 0/1875                       Loss D: 0.7136, loss G: 0.7562\n",
      "Epoch [159/50] Batch 0/1875                       Loss D: 0.6723, loss G: 0.7085\n",
      "Epoch [160/50] Batch 0/1875                       Loss D: 0.6374, loss G: 0.7302\n",
      "Epoch [161/50] Batch 0/1875                       Loss D: 0.7740, loss G: 0.6924\n",
      "Epoch [162/50] Batch 0/1875                       Loss D: 0.6957, loss G: 0.7781\n",
      "Epoch [163/50] Batch 0/1875                       Loss D: 0.6371, loss G: 0.8207\n",
      "Epoch [164/50] Batch 0/1875                       Loss D: 0.7392, loss G: 0.7103\n",
      "Epoch [165/50] Batch 0/1875                       Loss D: 0.6755, loss G: 0.7834\n",
      "Epoch [166/50] Batch 0/1875                       Loss D: 0.6764, loss G: 0.7426\n",
      "Epoch [167/50] Batch 0/1875                       Loss D: 0.6799, loss G: 0.7224\n",
      "Epoch [168/50] Batch 0/1875                       Loss D: 0.7070, loss G: 0.7142\n",
      "Epoch [169/50] Batch 0/1875                       Loss D: 0.6521, loss G: 0.8053\n",
      "Epoch [170/50] Batch 0/1875                       Loss D: 0.7531, loss G: 0.6590\n",
      "Epoch [171/50] Batch 0/1875                       Loss D: 0.7172, loss G: 0.7354\n",
      "Epoch [172/50] Batch 0/1875                       Loss D: 0.7072, loss G: 0.6825\n",
      "Epoch [173/50] Batch 0/1875                       Loss D: 0.6949, loss G: 0.7291\n",
      "Epoch [174/50] Batch 0/1875                       Loss D: 0.6417, loss G: 0.8167\n",
      "Epoch [175/50] Batch 0/1875                       Loss D: 0.7105, loss G: 0.7229\n",
      "Epoch [176/50] Batch 0/1875                       Loss D: 0.7052, loss G: 0.7502\n",
      "Epoch [177/50] Batch 0/1875                       Loss D: 0.7295, loss G: 0.6463\n",
      "Epoch [178/50] Batch 0/1875                       Loss D: 0.6705, loss G: 0.6996\n",
      "Epoch [179/50] Batch 0/1875                       Loss D: 0.7266, loss G: 0.7240\n",
      "Epoch [180/50] Batch 0/1875                       Loss D: 0.7192, loss G: 0.7338\n",
      "Epoch [181/50] Batch 0/1875                       Loss D: 0.7174, loss G: 0.6665\n",
      "Epoch [182/50] Batch 0/1875                       Loss D: 0.6956, loss G: 0.7679\n",
      "Epoch [183/50] Batch 0/1875                       Loss D: 0.7051, loss G: 0.6930\n",
      "Epoch [184/50] Batch 0/1875                       Loss D: 0.7553, loss G: 0.6498\n",
      "Epoch [185/50] Batch 0/1875                       Loss D: 0.7060, loss G: 0.7369\n",
      "Epoch [186/50] Batch 0/1875                       Loss D: 0.7251, loss G: 0.6656\n",
      "Epoch [187/50] Batch 0/1875                       Loss D: 0.6908, loss G: 0.7096\n",
      "Epoch [188/50] Batch 0/1875                       Loss D: 0.6863, loss G: 0.6850\n",
      "Epoch [189/50] Batch 0/1875                       Loss D: 0.7166, loss G: 0.6087\n",
      "Epoch [190/50] Batch 0/1875                       Loss D: 0.6610, loss G: 0.7370\n",
      "Epoch [191/50] Batch 0/1875                       Loss D: 0.6656, loss G: 0.7728\n",
      "Epoch [192/50] Batch 0/1875                       Loss D: 0.7336, loss G: 0.6428\n",
      "Epoch [193/50] Batch 0/1875                       Loss D: 0.7263, loss G: 0.6709\n",
      "Epoch [194/50] Batch 0/1875                       Loss D: 0.7303, loss G: 0.7406\n",
      "Epoch [195/50] Batch 0/1875                       Loss D: 0.6963, loss G: 0.6974\n",
      "Epoch [196/50] Batch 0/1875                       Loss D: 0.7003, loss G: 0.6567\n",
      "Epoch [197/50] Batch 0/1875                       Loss D: 0.7367, loss G: 0.6381\n",
      "Epoch [198/50] Batch 0/1875                       Loss D: 0.6944, loss G: 0.7758\n",
      "Epoch [199/50] Batch 0/1875                       Loss D: 0.6843, loss G: 0.7650\n",
      "Epoch [200/50] Batch 0/1875                       Loss D: 0.6772, loss G: 0.6905\n",
      "Epoch [201/50] Batch 0/1875                       Loss D: 0.6783, loss G: 0.7030\n",
      "Epoch [202/50] Batch 0/1875                       Loss D: 0.7042, loss G: 0.7145\n",
      "Epoch [203/50] Batch 0/1875                       Loss D: 0.6496, loss G: 0.7591\n",
      "Epoch [204/50] Batch 0/1875                       Loss D: 0.7103, loss G: 0.6544\n",
      "Epoch [205/50] Batch 0/1875                       Loss D: 0.7040, loss G: 0.6796\n",
      "Epoch [206/50] Batch 0/1875                       Loss D: 0.6716, loss G: 0.7333\n",
      "Epoch [207/50] Batch 0/1875                       Loss D: 0.6862, loss G: 0.7492\n",
      "Epoch [208/50] Batch 0/1875                       Loss D: 0.6788, loss G: 0.7000\n",
      "Epoch [209/50] Batch 0/1875                       Loss D: 0.7067, loss G: 0.7073\n",
      "Epoch [210/50] Batch 0/1875                       Loss D: 0.7030, loss G: 0.6703\n",
      "Epoch [211/50] Batch 0/1875                       Loss D: 0.6781, loss G: 0.7376\n",
      "Epoch [212/50] Batch 0/1875                       Loss D: 0.6942, loss G: 0.7162\n",
      "Epoch [213/50] Batch 0/1875                       Loss D: 0.6847, loss G: 0.6976\n",
      "Epoch [214/50] Batch 0/1875                       Loss D: 0.6906, loss G: 0.7083\n",
      "Epoch [215/50] Batch 0/1875                       Loss D: 0.7114, loss G: 0.6777\n",
      "Epoch [216/50] Batch 0/1875                       Loss D: 0.6644, loss G: 0.7029\n",
      "Epoch [217/50] Batch 0/1875                       Loss D: 0.7589, loss G: 0.6501\n",
      "Epoch [218/50] Batch 0/1875                       Loss D: 0.6803, loss G: 0.7224\n",
      "Epoch [219/50] Batch 0/1875                       Loss D: 0.6893, loss G: 0.6931\n",
      "Epoch [220/50] Batch 0/1875                       Loss D: 0.7292, loss G: 0.6684\n",
      "Epoch [221/50] Batch 0/1875                       Loss D: 0.7105, loss G: 0.6919\n",
      "Epoch [222/50] Batch 0/1875                       Loss D: 0.6754, loss G: 0.7294\n",
      "Epoch [223/50] Batch 0/1875                       Loss D: 0.7009, loss G: 0.7026\n",
      "Epoch [224/50] Batch 0/1875                       Loss D: 0.6533, loss G: 0.8064\n",
      "Epoch [225/50] Batch 0/1875                       Loss D: 0.6611, loss G: 0.7641\n",
      "Epoch [226/50] Batch 0/1875                       Loss D: 0.6792, loss G: 0.7311\n",
      "Epoch [227/50] Batch 0/1875                       Loss D: 0.6957, loss G: 0.7221\n",
      "Epoch [228/50] Batch 0/1875                       Loss D: 0.6988, loss G: 0.7019\n",
      "Epoch [229/50] Batch 0/1875                       Loss D: 0.6830, loss G: 0.7167\n",
      "Epoch [230/50] Batch 0/1875                       Loss D: 0.6774, loss G: 0.7277\n",
      "Epoch [231/50] Batch 0/1875                       Loss D: 0.7036, loss G: 0.7005\n",
      "Epoch [232/50] Batch 0/1875                       Loss D: 0.7107, loss G: 0.6957\n",
      "Epoch [233/50] Batch 0/1875                       Loss D: 0.6733, loss G: 0.7061\n",
      "Epoch [234/50] Batch 0/1875                       Loss D: 0.6751, loss G: 0.7237\n",
      "Epoch [235/50] Batch 0/1875                       Loss D: 0.7024, loss G: 0.6826\n",
      "Epoch [236/50] Batch 0/1875                       Loss D: 0.7241, loss G: 0.6750\n",
      "Epoch [237/50] Batch 0/1875                       Loss D: 0.6970, loss G: 0.6904\n",
      "Epoch [238/50] Batch 0/1875                       Loss D: 0.6803, loss G: 0.6995\n",
      "Epoch [239/50] Batch 0/1875                       Loss D: 0.7231, loss G: 0.6932\n",
      "Epoch [240/50] Batch 0/1875                       Loss D: 0.6773, loss G: 0.6847\n",
      "Epoch [241/50] Batch 0/1875                       Loss D: 0.6852, loss G: 0.7139\n",
      "Epoch [242/50] Batch 0/1875                       Loss D: 0.6929, loss G: 0.7140\n",
      "Epoch [243/50] Batch 0/1875                       Loss D: 0.5138, loss G: 0.9364\n",
      "Epoch [244/50] Batch 0/1875                       Loss D: 0.6969, loss G: 0.7050\n",
      "Epoch [245/50] Batch 0/1875                       Loss D: 0.7143, loss G: 0.6808\n",
      "Epoch [246/50] Batch 0/1875                       Loss D: 0.7380, loss G: 0.6747\n",
      "Epoch [247/50] Batch 0/1875                       Loss D: 0.7206, loss G: 0.6674\n",
      "Epoch [248/50] Batch 0/1875                       Loss D: 0.7193, loss G: 0.6887\n",
      "Epoch [249/50] Batch 0/1875                       Loss D: 0.6804, loss G: 0.7311\n",
      "Epoch [250/50] Batch 0/1875                       Loss D: 0.6650, loss G: 0.7309\n",
      "Epoch [251/50] Batch 0/1875                       Loss D: 0.6420, loss G: 0.7882\n",
      "Epoch [252/50] Batch 0/1875                       Loss D: 0.6813, loss G: 0.7233\n",
      "Epoch [253/50] Batch 0/1875                       Loss D: 0.6989, loss G: 0.7034\n",
      "Epoch [254/50] Batch 0/1875                       Loss D: 0.7386, loss G: 0.6394\n",
      "Epoch [255/50] Batch 0/1875                       Loss D: 0.6671, loss G: 0.7196\n",
      "Epoch [256/50] Batch 0/1875                       Loss D: 0.7030, loss G: 0.7003\n",
      "Epoch [257/50] Batch 0/1875                       Loss D: 0.7551, loss G: 0.6127\n",
      "Epoch [258/50] Batch 0/1875                       Loss D: 0.7027, loss G: 0.6959\n",
      "Epoch [259/50] Batch 0/1875                       Loss D: 0.7001, loss G: 0.6933\n",
      "Epoch [260/50] Batch 0/1875                       Loss D: 0.7256, loss G: 0.6757\n",
      "Epoch [261/50] Batch 0/1875                       Loss D: 0.7485, loss G: 0.6498\n",
      "Epoch [262/50] Batch 0/1875                       Loss D: 0.7012, loss G: 0.6959\n",
      "Epoch [263/50] Batch 0/1875                       Loss D: 0.6786, loss G: 0.7582\n",
      "Epoch [264/50] Batch 0/1875                       Loss D: 0.7180, loss G: 0.6464\n",
      "Epoch [265/50] Batch 0/1875                       Loss D: 0.7028, loss G: 0.6969\n",
      "Epoch [266/50] Batch 0/1875                       Loss D: 0.7090, loss G: 0.7087\n",
      "Epoch [267/50] Batch 0/1875                       Loss D: 0.7125, loss G: 0.7119\n",
      "Epoch [268/50] Batch 0/1875                       Loss D: 0.7454, loss G: 0.6437\n",
      "Epoch [269/50] Batch 0/1875                       Loss D: 0.6391, loss G: 0.7772\n",
      "Epoch [270/50] Batch 0/1875                       Loss D: 0.6865, loss G: 0.7350\n",
      "Epoch [271/50] Batch 0/1875                       Loss D: 0.7899, loss G: 0.6264\n",
      "Epoch [272/50] Batch 0/1875                       Loss D: 0.7185, loss G: 0.6700\n",
      "Epoch [273/50] Batch 0/1875                       Loss D: 0.7049, loss G: 0.6753\n",
      "Epoch [274/50] Batch 0/1875                       Loss D: 0.7230, loss G: 0.6708\n",
      "Epoch [275/50] Batch 0/1875                       Loss D: 0.6566, loss G: 0.7472\n",
      "Epoch [276/50] Batch 0/1875                       Loss D: 0.7032, loss G: 0.7119\n",
      "Epoch [277/50] Batch 0/1875                       Loss D: 0.7054, loss G: 0.7176\n",
      "Epoch [278/50] Batch 0/1875                       Loss D: 0.7560, loss G: 0.6249\n",
      "Epoch [279/50] Batch 0/1875                       Loss D: 0.7354, loss G: 0.6617\n",
      "Epoch [280/50] Batch 0/1875                       Loss D: 0.8039, loss G: 0.6046\n",
      "Epoch [281/50] Batch 0/1875                       Loss D: 0.6438, loss G: 0.7615\n",
      "Epoch [282/50] Batch 0/1875                       Loss D: 0.6613, loss G: 0.7936\n",
      "Epoch [283/50] Batch 0/1875                       Loss D: 0.6131, loss G: 0.7836\n",
      "Epoch [284/50] Batch 0/1875                       Loss D: 0.7180, loss G: 0.7072\n",
      "Epoch [285/50] Batch 0/1875                       Loss D: 0.7212, loss G: 0.6796\n",
      "Epoch [286/50] Batch 0/1875                       Loss D: 0.6830, loss G: 0.7453\n",
      "Epoch [287/50] Batch 0/1875                       Loss D: 0.6504, loss G: 0.7440\n",
      "Epoch [288/50] Batch 0/1875                       Loss D: 0.6341, loss G: 0.7931\n",
      "Epoch [289/50] Batch 0/1875                       Loss D: 0.6514, loss G: 0.7683\n",
      "Epoch [290/50] Batch 0/1875                       Loss D: 0.7724, loss G: 0.6191\n",
      "Epoch [291/50] Batch 0/1875                       Loss D: 0.6020, loss G: 0.8213\n",
      "Epoch [292/50] Batch 0/1875                       Loss D: 0.7666, loss G: 0.6595\n",
      "Epoch [293/50] Batch 0/1875                       Loss D: 0.6359, loss G: 0.7896\n",
      "Epoch [294/50] Batch 0/1875                       Loss D: 0.6857, loss G: 0.7100\n",
      "Epoch [295/50] Batch 0/1875                       Loss D: 0.6730, loss G: 0.7215\n",
      "Epoch [296/50] Batch 0/1875                       Loss D: 0.7056, loss G: 0.6990\n",
      "Epoch [297/50] Batch 0/1875                       Loss D: 0.7126, loss G: 0.6875\n",
      "Epoch [298/50] Batch 0/1875                       Loss D: 0.6991, loss G: 0.7389\n",
      "Epoch [299/50] Batch 0/1875                       Loss D: 0.6806, loss G: 0.7414\n",
      "Epoch [300/50] Batch 0/1875                       Loss D: 0.7304, loss G: 0.6949\n",
      "Epoch [301/50] Batch 0/1875                       Loss D: 0.7339, loss G: 0.6552\n",
      "Epoch [302/50] Batch 0/1875                       Loss D: 0.6944, loss G: 0.7080\n",
      "Epoch [303/50] Batch 0/1875                       Loss D: 0.6981, loss G: 0.6966\n",
      "Epoch [304/50] Batch 0/1875                       Loss D: 0.6720, loss G: 0.7068\n",
      "Epoch [305/50] Batch 0/1875                       Loss D: 0.6560, loss G: 0.7566\n",
      "Epoch [306/50] Batch 0/1875                       Loss D: 0.7483, loss G: 0.6599\n",
      "Epoch [307/50] Batch 0/1875                       Loss D: 0.7427, loss G: 0.6887\n",
      "Epoch [308/50] Batch 0/1875                       Loss D: 0.7177, loss G: 0.7224\n",
      "Epoch [309/50] Batch 0/1875                       Loss D: 0.6397, loss G: 0.8388\n",
      "Epoch [310/50] Batch 0/1875                       Loss D: 0.6824, loss G: 0.7316\n",
      "Epoch [311/50] Batch 0/1875                       Loss D: 0.7740, loss G: 0.6501\n",
      "Epoch [312/50] Batch 0/1875                       Loss D: 0.6304, loss G: 0.7686\n",
      "Epoch [313/50] Batch 0/1875                       Loss D: 0.6493, loss G: 0.7764\n",
      "Epoch [314/50] Batch 0/1875                       Loss D: 0.7818, loss G: 0.6094\n",
      "Epoch [315/50] Batch 0/1875                       Loss D: 0.7066, loss G: 0.7164\n",
      "Epoch [316/50] Batch 0/1875                       Loss D: 0.7155, loss G: 0.6488\n",
      "Epoch [317/50] Batch 0/1875                       Loss D: 0.7204, loss G: 0.7088\n",
      "Epoch [318/50] Batch 0/1875                       Loss D: 0.7164, loss G: 0.6881\n",
      "Epoch [319/50] Batch 0/1875                       Loss D: 0.6396, loss G: 0.7650\n",
      "Epoch [320/50] Batch 0/1875                       Loss D: 0.6910, loss G: 0.7511\n",
      "Epoch [321/50] Batch 0/1875                       Loss D: 0.7093, loss G: 0.6810\n",
      "Epoch [322/50] Batch 0/1875                       Loss D: 0.6394, loss G: 0.7745\n",
      "Epoch [323/50] Batch 0/1875                       Loss D: 0.6366, loss G: 0.7819\n",
      "Epoch [324/50] Batch 0/1875                       Loss D: 0.7533, loss G: 0.6338\n",
      "Epoch [325/50] Batch 0/1875                       Loss D: 0.6157, loss G: 0.7885\n",
      "Epoch [326/50] Batch 0/1875                       Loss D: 0.7243, loss G: 0.6820\n",
      "Epoch [327/50] Batch 0/1875                       Loss D: 0.7309, loss G: 0.6748\n",
      "Epoch [328/50] Batch 0/1875                       Loss D: 0.7018, loss G: 0.7010\n",
      "Epoch [329/50] Batch 0/1875                       Loss D: 0.6905, loss G: 0.7038\n",
      "Epoch [330/50] Batch 0/1875                       Loss D: 0.7313, loss G: 0.6527\n",
      "Epoch [331/50] Batch 0/1875                       Loss D: 0.7538, loss G: 0.6473\n",
      "Epoch [332/50] Batch 0/1875                       Loss D: 0.7401, loss G: 0.6622\n",
      "Epoch [333/50] Batch 0/1875                       Loss D: 0.6892, loss G: 0.7182\n",
      "Epoch [334/50] Batch 0/1875                       Loss D: 0.7357, loss G: 0.6486\n",
      "Epoch [335/50] Batch 0/1875                       Loss D: 0.6295, loss G: 0.7582\n",
      "Epoch [336/50] Batch 0/1875                       Loss D: 0.6443, loss G: 0.7574\n",
      "Epoch [337/50] Batch 0/1875                       Loss D: 0.6923, loss G: 0.6833\n",
      "Epoch [338/50] Batch 0/1875                       Loss D: 0.6478, loss G: 0.7962\n",
      "Epoch [339/50] Batch 0/1875                       Loss D: 0.6964, loss G: 0.7071\n",
      "Epoch [340/50] Batch 0/1875                       Loss D: 0.7276, loss G: 0.6752\n",
      "Epoch [341/50] Batch 0/1875                       Loss D: 0.6991, loss G: 0.7222\n",
      "Epoch [342/50] Batch 0/1875                       Loss D: 0.7719, loss G: 0.6573\n",
      "Epoch [343/50] Batch 0/1875                       Loss D: 0.6685, loss G: 0.7396\n",
      "Epoch [344/50] Batch 0/1875                       Loss D: 0.6902, loss G: 0.7720\n",
      "Epoch [345/50] Batch 0/1875                       Loss D: 0.6951, loss G: 0.7055\n",
      "Epoch [346/50] Batch 0/1875                       Loss D: 0.6735, loss G: 0.7421\n",
      "Epoch [347/50] Batch 0/1875                       Loss D: 0.7731, loss G: 0.6168\n",
      "Epoch [348/50] Batch 0/1875                       Loss D: 0.7232, loss G: 0.6665\n",
      "Epoch [349/50] Batch 0/1875                       Loss D: 0.7204, loss G: 0.6996\n",
      "Epoch [350/50] Batch 0/1875                       Loss D: 0.7278, loss G: 0.6776\n",
      "Epoch [351/50] Batch 0/1875                       Loss D: 0.6800, loss G: 0.7355\n",
      "Epoch [352/50] Batch 0/1875                       Loss D: 0.6835, loss G: 0.7027\n",
      "Epoch [353/50] Batch 0/1875                       Loss D: 0.7555, loss G: 0.6172\n",
      "Epoch [354/50] Batch 0/1875                       Loss D: 0.7113, loss G: 0.6850\n",
      "Epoch [355/50] Batch 0/1875                       Loss D: 0.6334, loss G: 0.7523\n",
      "Epoch [356/50] Batch 0/1875                       Loss D: 0.7755, loss G: 0.6473\n",
      "Epoch [357/50] Batch 0/1875                       Loss D: 0.6973, loss G: 0.7150\n",
      "Epoch [358/50] Batch 0/1875                       Loss D: 0.7209, loss G: 0.6579\n",
      "Epoch [359/50] Batch 0/1875                       Loss D: 0.6785, loss G: 0.7274\n",
      "Epoch [360/50] Batch 0/1875                       Loss D: 0.6902, loss G: 0.6902\n",
      "Epoch [361/50] Batch 0/1875                       Loss D: 0.6903, loss G: 0.7244\n",
      "Epoch [362/50] Batch 0/1875                       Loss D: 0.6935, loss G: 0.7299\n",
      "Epoch [363/50] Batch 0/1875                       Loss D: 0.6915, loss G: 0.6725\n",
      "Epoch [364/50] Batch 0/1875                       Loss D: 0.6745, loss G: 0.7155\n",
      "Epoch [365/50] Batch 0/1875                       Loss D: 0.6863, loss G: 0.6906\n",
      "Epoch [366/50] Batch 0/1875                       Loss D: 0.7220, loss G: 0.6879\n",
      "Epoch [367/50] Batch 0/1875                       Loss D: 0.6607, loss G: 0.7675\n",
      "Epoch [368/50] Batch 0/1875                       Loss D: 0.6884, loss G: 0.7073\n",
      "Epoch [369/50] Batch 0/1875                       Loss D: 0.6985, loss G: 0.7206\n",
      "Epoch [370/50] Batch 0/1875                       Loss D: 0.6385, loss G: 0.7246\n",
      "Epoch [371/50] Batch 0/1875                       Loss D: 0.7531, loss G: 0.6553\n",
      "Epoch [372/50] Batch 0/1875                       Loss D: 0.6859, loss G: 0.7120\n",
      "Epoch [373/50] Batch 0/1875                       Loss D: 0.7325, loss G: 0.6691\n",
      "Epoch [374/50] Batch 0/1875                       Loss D: 0.7423, loss G: 0.6466\n",
      "Epoch [375/50] Batch 0/1875                       Loss D: 0.7182, loss G: 0.7033\n",
      "Epoch [376/50] Batch 0/1875                       Loss D: 0.6858, loss G: 0.7240\n",
      "Epoch [377/50] Batch 0/1875                       Loss D: 0.6359, loss G: 0.7772\n",
      "Epoch [378/50] Batch 0/1875                       Loss D: 0.6937, loss G: 0.6883\n",
      "Epoch [379/50] Batch 0/1875                       Loss D: 0.6627, loss G: 0.7425\n",
      "Epoch [380/50] Batch 0/1875                       Loss D: 0.6942, loss G: 0.6909\n",
      "Epoch [381/50] Batch 0/1875                       Loss D: 0.6719, loss G: 0.7147\n",
      "Epoch [382/50] Batch 0/1875                       Loss D: 0.6894, loss G: 0.7076\n",
      "Epoch [383/50] Batch 0/1875                       Loss D: 0.7287, loss G: 0.6702\n",
      "Epoch [384/50] Batch 0/1875                       Loss D: 0.6842, loss G: 0.6879\n",
      "Epoch [385/50] Batch 0/1875                       Loss D: 0.7914, loss G: 0.6002\n",
      "Epoch [386/50] Batch 0/1875                       Loss D: 0.6148, loss G: 0.8073\n",
      "Epoch [387/50] Batch 0/1875                       Loss D: 0.6619, loss G: 0.7538\n",
      "Epoch [388/50] Batch 0/1875                       Loss D: 0.7751, loss G: 0.6134\n",
      "Epoch [389/50] Batch 0/1875                       Loss D: 0.7198, loss G: 0.6867\n",
      "Epoch [390/50] Batch 0/1875                       Loss D: 0.7015, loss G: 0.7259\n",
      "Epoch [391/50] Batch 0/1875                       Loss D: 0.7149, loss G: 0.6954\n",
      "Epoch [392/50] Batch 0/1875                       Loss D: 0.6912, loss G: 0.7269\n",
      "Epoch [393/50] Batch 0/1875                       Loss D: 0.7158, loss G: 0.6833\n",
      "Epoch [394/50] Batch 0/1875                       Loss D: 0.7231, loss G: 0.6895\n",
      "Epoch [395/50] Batch 0/1875                       Loss D: 0.6707, loss G: 0.7255\n",
      "Epoch [396/50] Batch 0/1875                       Loss D: 0.7427, loss G: 0.6508\n",
      "Epoch [397/50] Batch 0/1875                       Loss D: 0.6437, loss G: 0.7759\n",
      "Epoch [398/50] Batch 0/1875                       Loss D: 0.7410, loss G: 0.6567\n",
      "Epoch [399/50] Batch 0/1875                       Loss D: 0.6614, loss G: 0.7706\n",
      "Epoch [400/50] Batch 0/1875                       Loss D: 0.6882, loss G: 0.7014\n",
      "Epoch [401/50] Batch 0/1875                       Loss D: 0.6860, loss G: 0.7082\n",
      "Epoch [402/50] Batch 0/1875                       Loss D: 0.6871, loss G: 0.7172\n",
      "Epoch [403/50] Batch 0/1875                       Loss D: 0.7428, loss G: 0.6676\n",
      "Epoch [404/50] Batch 0/1875                       Loss D: 0.6779, loss G: 0.7459\n",
      "Epoch [405/50] Batch 0/1875                       Loss D: 0.7200, loss G: 0.6683\n",
      "Epoch [406/50] Batch 0/1875                       Loss D: 0.6419, loss G: 0.7913\n",
      "Epoch [407/50] Batch 0/1875                       Loss D: 0.6840, loss G: 0.7330\n",
      "Epoch [408/50] Batch 0/1875                       Loss D: 0.7329, loss G: 0.6663\n",
      "Epoch [409/50] Batch 0/1875                       Loss D: 0.7435, loss G: 0.6618\n",
      "Epoch [410/50] Batch 0/1875                       Loss D: 0.6311, loss G: 0.7687\n",
      "Epoch [411/50] Batch 0/1875                       Loss D: 0.7219, loss G: 0.7030\n",
      "Epoch [412/50] Batch 0/1875                       Loss D: 0.6835, loss G: 0.7005\n",
      "Epoch [413/50] Batch 0/1875                       Loss D: 0.6840, loss G: 0.7182\n",
      "Epoch [414/50] Batch 0/1875                       Loss D: 0.6697, loss G: 0.7200\n",
      "Epoch [415/50] Batch 0/1875                       Loss D: 0.7065, loss G: 0.7121\n",
      "Epoch [416/50] Batch 0/1875                       Loss D: 0.7394, loss G: 0.6811\n",
      "Epoch [417/50] Batch 0/1875                       Loss D: 0.7249, loss G: 0.7133\n",
      "Epoch [418/50] Batch 0/1875                       Loss D: 0.7522, loss G: 0.6921\n",
      "Epoch [419/50] Batch 0/1875                       Loss D: 0.7151, loss G: 0.6934\n",
      "Epoch [420/50] Batch 0/1875                       Loss D: 0.5942, loss G: 0.8270\n",
      "Epoch [421/50] Batch 0/1875                       Loss D: 0.6511, loss G: 0.7260\n",
      "Epoch [422/50] Batch 0/1875                       Loss D: 0.6851, loss G: 0.7121\n",
      "Epoch [423/50] Batch 0/1875                       Loss D: 0.7368, loss G: 0.6704\n",
      "Epoch [424/50] Batch 0/1875                       Loss D: 0.6616, loss G: 0.7832\n",
      "Epoch [425/50] Batch 0/1875                       Loss D: 0.6932, loss G: 0.6989\n",
      "Epoch [426/50] Batch 0/1875                       Loss D: 0.6682, loss G: 0.7325\n",
      "Epoch [427/50] Batch 0/1875                       Loss D: 0.7103, loss G: 0.6999\n",
      "Epoch [428/50] Batch 0/1875                       Loss D: 0.6298, loss G: 0.8157\n",
      "Epoch [429/50] Batch 0/1875                       Loss D: 0.6630, loss G: 0.7518\n",
      "Epoch [430/50] Batch 0/1875                       Loss D: 0.7175, loss G: 0.6666\n",
      "Epoch [431/50] Batch 0/1875                       Loss D: 0.7078, loss G: 0.7264\n",
      "Epoch [432/50] Batch 0/1875                       Loss D: 0.7045, loss G: 0.6721\n",
      "Epoch [433/50] Batch 0/1875                       Loss D: 0.5757, loss G: 0.8666\n",
      "Epoch [434/50] Batch 0/1875                       Loss D: 0.6738, loss G: 0.7344\n",
      "Epoch [435/50] Batch 0/1875                       Loss D: 0.7467, loss G: 0.6816\n",
      "Epoch [436/50] Batch 0/1875                       Loss D: 0.6691, loss G: 0.7038\n",
      "Epoch [437/50] Batch 0/1875                       Loss D: 0.6752, loss G: 0.7276\n",
      "Epoch [438/50] Batch 0/1875                       Loss D: 0.6646, loss G: 0.7070\n",
      "Epoch [439/50] Batch 0/1875                       Loss D: 0.6112, loss G: 0.8228\n",
      "Epoch [440/50] Batch 0/1875                       Loss D: 0.7556, loss G: 0.6706\n",
      "Epoch [441/50] Batch 0/1875                       Loss D: 0.7063, loss G: 0.6710\n",
      "Epoch [442/50] Batch 0/1875                       Loss D: 0.7259, loss G: 0.6563\n",
      "Epoch [443/50] Batch 0/1875                       Loss D: 0.7389, loss G: 0.6769\n",
      "Epoch [444/50] Batch 0/1875                       Loss D: 0.7594, loss G: 0.6084\n",
      "Epoch [445/50] Batch 0/1875                       Loss D: 0.6771, loss G: 0.7309\n",
      "Epoch [446/50] Batch 0/1875                       Loss D: 0.7233, loss G: 0.6878\n",
      "Epoch [447/50] Batch 0/1875                       Loss D: 0.6877, loss G: 0.7114\n",
      "Epoch [448/50] Batch 0/1875                       Loss D: 0.6285, loss G: 0.7974\n",
      "Epoch [449/50] Batch 0/1875                       Loss D: 0.7128, loss G: 0.6822\n",
      "Epoch [450/50] Batch 0/1875                       Loss D: 0.7032, loss G: 0.6912\n",
      "Epoch [451/50] Batch 0/1875                       Loss D: 0.6649, loss G: 0.7780\n",
      "Epoch [452/50] Batch 0/1875                       Loss D: 0.6998, loss G: 0.7203\n",
      "Epoch [453/50] Batch 0/1875                       Loss D: 0.7360, loss G: 0.6859\n",
      "Epoch [454/50] Batch 0/1875                       Loss D: 0.7254, loss G: 0.6622\n",
      "Epoch [455/50] Batch 0/1875                       Loss D: 0.6989, loss G: 0.6840\n",
      "Epoch [456/50] Batch 0/1875                       Loss D: 0.6677, loss G: 0.7375\n",
      "Epoch [457/50] Batch 0/1875                       Loss D: 0.7035, loss G: 0.7339\n",
      "Epoch [458/50] Batch 0/1875                       Loss D: 0.6748, loss G: 0.7100\n",
      "Epoch [459/50] Batch 0/1875                       Loss D: 0.7244, loss G: 0.6865\n",
      "Epoch [460/50] Batch 0/1875                       Loss D: 0.6999, loss G: 0.6959\n",
      "Epoch [461/50] Batch 0/1875                       Loss D: 0.7398, loss G: 0.6553\n",
      "Epoch [462/50] Batch 0/1875                       Loss D: 0.6516, loss G: 0.7124\n",
      "Epoch [463/50] Batch 0/1875                       Loss D: 0.6807, loss G: 0.7012\n",
      "Epoch [464/50] Batch 0/1875                       Loss D: 0.7849, loss G: 0.6113\n",
      "Epoch [465/50] Batch 0/1875                       Loss D: 0.6668, loss G: 0.7472\n",
      "Epoch [466/50] Batch 0/1875                       Loss D: 0.6858, loss G: 0.6898\n",
      "Epoch [467/50] Batch 0/1875                       Loss D: 0.6311, loss G: 0.7737\n",
      "Epoch [468/50] Batch 0/1875                       Loss D: 0.7222, loss G: 0.6572\n",
      "Epoch [469/50] Batch 0/1875                       Loss D: 0.7407, loss G: 0.6599\n",
      "Epoch [470/50] Batch 0/1875                       Loss D: 0.7178, loss G: 0.6967\n",
      "Epoch [471/50] Batch 0/1875                       Loss D: 0.6742, loss G: 0.7209\n",
      "Epoch [472/50] Batch 0/1875                       Loss D: 0.8078, loss G: 0.5903\n",
      "Epoch [473/50] Batch 0/1875                       Loss D: 0.7226, loss G: 0.6729\n",
      "Epoch [474/50] Batch 0/1875                       Loss D: 0.7399, loss G: 0.6827\n",
      "Epoch [475/50] Batch 0/1875                       Loss D: 0.7204, loss G: 0.6902\n",
      "Epoch [476/50] Batch 0/1875                       Loss D: 0.7166, loss G: 0.6743\n",
      "Epoch [477/50] Batch 0/1875                       Loss D: 0.6365, loss G: 0.8081\n",
      "Epoch [478/50] Batch 0/1875                       Loss D: 0.6920, loss G: 0.7252\n",
      "Epoch [479/50] Batch 0/1875                       Loss D: 0.7113, loss G: 0.7318\n",
      "Epoch [480/50] Batch 0/1875                       Loss D: 0.7549, loss G: 0.6449\n",
      "Epoch [481/50] Batch 0/1875                       Loss D: 0.6512, loss G: 0.7744\n",
      "Epoch [482/50] Batch 0/1875                       Loss D: 0.6295, loss G: 0.7871\n",
      "Epoch [483/50] Batch 0/1875                       Loss D: 0.7152, loss G: 0.6712\n",
      "Epoch [484/50] Batch 0/1875                       Loss D: 0.8110, loss G: 0.5815\n",
      "Epoch [485/50] Batch 0/1875                       Loss D: 0.6797, loss G: 0.7261\n",
      "Epoch [486/50] Batch 0/1875                       Loss D: 0.6490, loss G: 0.7557\n",
      "Epoch [487/50] Batch 0/1875                       Loss D: 0.6965, loss G: 0.6740\n",
      "Epoch [488/50] Batch 0/1875                       Loss D: 0.6684, loss G: 0.7705\n",
      "Epoch [489/50] Batch 0/1875                       Loss D: 0.7023, loss G: 0.6952\n",
      "Epoch [490/50] Batch 0/1875                       Loss D: 0.6886, loss G: 0.7356\n",
      "Epoch [491/50] Batch 0/1875                       Loss D: 0.7050, loss G: 0.7036\n",
      "Epoch [492/50] Batch 0/1875                       Loss D: 0.6999, loss G: 0.7000\n",
      "Epoch [493/50] Batch 0/1875                       Loss D: 0.7054, loss G: 0.6904\n",
      "Epoch [494/50] Batch 0/1875                       Loss D: 0.7286, loss G: 0.6668\n",
      "Epoch [495/50] Batch 0/1875                       Loss D: 0.7040, loss G: 0.7002\n",
      "Epoch [496/50] Batch 0/1875                       Loss D: 0.6771, loss G: 0.7510\n",
      "Epoch [497/50] Batch 0/1875                       Loss D: 0.7048, loss G: 0.7223\n",
      "Epoch [498/50] Batch 0/1875                       Loss D: 0.6902, loss G: 0.7146\n",
      "Epoch [499/50] Batch 0/1875                       Loss D: 0.6878, loss G: 0.7152\n",
      "Epoch [500/50] Batch 0/1875                       Loss D: 0.6340, loss G: 0.7558\n",
      "Epoch [501/50] Batch 0/1875                       Loss D: 0.7216, loss G: 0.6936\n",
      "Epoch [502/50] Batch 0/1875                       Loss D: 0.7092, loss G: 0.6754\n",
      "Epoch [503/50] Batch 0/1875                       Loss D: 0.7373, loss G: 0.6586\n",
      "Epoch [504/50] Batch 0/1875                       Loss D: 0.6557, loss G: 0.7628\n",
      "Epoch [505/50] Batch 0/1875                       Loss D: 0.7470, loss G: 0.6231\n",
      "Epoch [506/50] Batch 0/1875                       Loss D: 0.8054, loss G: 0.6246\n",
      "Epoch [507/50] Batch 0/1875                       Loss D: 0.7128, loss G: 0.7525\n",
      "Epoch [508/50] Batch 0/1875                       Loss D: 0.7634, loss G: 0.6413\n",
      "Epoch [509/50] Batch 0/1875                       Loss D: 0.6032, loss G: 0.8064\n",
      "Epoch [510/50] Batch 0/1875                       Loss D: 0.6593, loss G: 0.7622\n",
      "Epoch [511/50] Batch 0/1875                       Loss D: 0.6236, loss G: 0.8424\n",
      "Epoch [512/50] Batch 0/1875                       Loss D: 0.7933, loss G: 0.6908\n",
      "Epoch [513/50] Batch 0/1875                       Loss D: 0.7358, loss G: 0.6851\n",
      "Epoch [514/50] Batch 0/1875                       Loss D: 0.8443, loss G: 0.6011\n",
      "Epoch [515/50] Batch 0/1875                       Loss D: 0.7622, loss G: 0.6484\n",
      "Epoch [516/50] Batch 0/1875                       Loss D: 0.6888, loss G: 0.7567\n",
      "Epoch [517/50] Batch 0/1875                       Loss D: 0.6942, loss G: 0.7535\n",
      "Epoch [518/50] Batch 0/1875                       Loss D: 0.7452, loss G: 0.7619\n",
      "Epoch [519/50] Batch 0/1875                       Loss D: 0.6228, loss G: 0.7905\n",
      "Epoch [520/50] Batch 0/1875                       Loss D: 0.6345, loss G: 0.7864\n",
      "Epoch [521/50] Batch 0/1875                       Loss D: 0.6743, loss G: 0.7092\n",
      "Epoch [522/50] Batch 0/1875                       Loss D: 0.8032, loss G: 0.6760\n",
      "Epoch [523/50] Batch 0/1875                       Loss D: 0.6759, loss G: 0.7591\n",
      "Epoch [524/50] Batch 0/1875                       Loss D: 0.6951, loss G: 0.7544\n",
      "Epoch [525/50] Batch 0/1875                       Loss D: 0.7477, loss G: 0.7046\n",
      "Epoch [526/50] Batch 0/1875                       Loss D: 1.0365, loss G: 0.4636\n",
      "Epoch [527/50] Batch 0/1875                       Loss D: 0.6986, loss G: 0.6965\n",
      "Epoch [528/50] Batch 0/1875                       Loss D: 0.7179, loss G: 0.6726\n",
      "Epoch [529/50] Batch 0/1875                       Loss D: 0.7171, loss G: 0.7670\n",
      "Epoch [530/50] Batch 0/1875                       Loss D: 0.6916, loss G: 0.7140\n",
      "Epoch [531/50] Batch 0/1875                       Loss D: 0.8002, loss G: 0.6130\n",
      "Epoch [532/50] Batch 0/1875                       Loss D: 0.7273, loss G: 0.6723\n",
      "Epoch [533/50] Batch 0/1875                       Loss D: 0.7795, loss G: 0.6156\n",
      "Epoch [534/50] Batch 0/1875                       Loss D: 0.7963, loss G: 0.6112\n",
      "Epoch [535/50] Batch 0/1875                       Loss D: 0.6919, loss G: 0.6795\n",
      "Epoch [536/50] Batch 0/1875                       Loss D: 0.7440, loss G: 0.7318\n",
      "Epoch [537/50] Batch 0/1875                       Loss D: 0.7493, loss G: 0.6804\n",
      "Epoch [538/50] Batch 0/1875                       Loss D: 0.7219, loss G: 0.7069\n",
      "Epoch [539/50] Batch 0/1875                       Loss D: 0.7298, loss G: 0.6578\n",
      "Epoch [540/50] Batch 0/1875                       Loss D: 0.7557, loss G: 0.6463\n",
      "Epoch [541/50] Batch 0/1875                       Loss D: 0.6113, loss G: 0.8499\n",
      "Epoch [542/50] Batch 0/1875                       Loss D: 0.6927, loss G: 0.6883\n",
      "Epoch [543/50] Batch 0/1875                       Loss D: 0.6293, loss G: 0.8146\n",
      "Epoch [544/50] Batch 0/1875                       Loss D: 0.7671, loss G: 0.6186\n",
      "Epoch [545/50] Batch 0/1875                       Loss D: 0.6675, loss G: 0.8313\n",
      "Epoch [546/50] Batch 0/1875                       Loss D: 0.7071, loss G: 0.7120\n",
      "Epoch [547/50] Batch 0/1875                       Loss D: 0.7157, loss G: 0.7197\n",
      "Epoch [548/50] Batch 0/1875                       Loss D: 0.7441, loss G: 0.6850\n",
      "Epoch [549/50] Batch 0/1875                       Loss D: 0.7867, loss G: 0.6307\n",
      "Epoch [550/50] Batch 0/1875                       Loss D: 0.8463, loss G: 0.6098\n",
      "Epoch [551/50] Batch 0/1875                       Loss D: 0.6816, loss G: 0.7664\n",
      "Epoch [552/50] Batch 0/1875                       Loss D: 0.6925, loss G: 0.6665\n",
      "Epoch [553/50] Batch 0/1875                       Loss D: 0.6930, loss G: 0.6873\n",
      "Epoch [554/50] Batch 0/1875                       Loss D: 0.6873, loss G: 0.6720\n",
      "Epoch [555/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6932\n",
      "Epoch [556/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [557/50] Batch 0/1875                       Loss D: 0.6886, loss G: 0.7156\n",
      "Epoch [558/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [559/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [560/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [561/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [562/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [563/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6933\n",
      "Epoch [564/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [565/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [566/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [567/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [568/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [569/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [570/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [571/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [572/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [573/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [574/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [575/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [576/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [577/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [578/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [579/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [580/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [581/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [582/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [583/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [584/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [585/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [586/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [587/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [588/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [589/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [590/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6930\n",
      "Epoch [591/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [592/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [593/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [594/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6932\n",
      "Epoch [595/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [596/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [597/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [598/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [599/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [600/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [601/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [602/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [603/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [604/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [605/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [606/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [607/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [608/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [609/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [610/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [611/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [612/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [613/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [614/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [615/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6932\n",
      "Epoch [616/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [617/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6932\n",
      "Epoch [618/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [619/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [620/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [621/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [622/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [623/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [624/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [625/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [626/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [627/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n",
      "Epoch [628/50] Batch 0/1875                       Loss D: 0.6931, loss G: 0.6931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-7:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\db\\.conda\\envs\\3.7_imagetuto\\lib\\threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\db\\.conda\\envs\\3.7_imagetuto\\lib\\site-packages\\tensorboard\\summary\\writer\\event_file_writer.py\", line 233, in run\n",
      "    self._record_writer.write(data)\n",
      "  File \"C:\\Users\\db\\.conda\\envs\\3.7_imagetuto\\lib\\site-packages\\tensorboard\\summary\\writer\\record_writer.py\", line 40, in write\n",
      "    self._writer.write(header + header_crc + data + footer_crc)\n",
      "  File \"C:\\Users\\db\\.conda\\envs\\3.7_imagetuto\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\", line 100, in write\n",
      "    self._writable_file.append(compat.as_bytes(file_content))\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xb5 in position 98: invalid start byte\n",
      "\n",
      "Exception in thread Thread-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\db\\.conda\\envs\\3.7_imagetuto\\lib\\threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\db\\.conda\\envs\\3.7_imagetuto\\lib\\site-packages\\tensorboard\\summary\\writer\\event_file_writer.py\", line 233, in run\n",
      "    self._record_writer.write(data)\n",
      "  File \"C:\\Users\\db\\.conda\\envs\\3.7_imagetuto\\lib\\site-packages\\tensorboard\\summary\\writer\\record_writer.py\", line 40, in write\n",
      "    self._writer.write(header + header_crc + data + footer_crc)\n",
      "  File \"C:\\Users\\db\\.conda\\envs\\3.7_imagetuto\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\", line 100, in write\n",
      "    self._writable_file.append(compat.as_bytes(file_content))\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xb5 in position 98: invalid start byte\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3000):\n",
    "    for batch_idx, (real,_) in enumerate(loader):\n",
    "        real = real.view(-1,784).to(device)\n",
    "        batch_size = real.shape[0]\n",
    "      \n",
    "        # noise 배치 x z_dim \n",
    "        noise = torch.randn(batch_size,z_dim).to(device)\n",
    "        fake = gen(noise)\n",
    "        # 진짜 이미지는 1로 학습시킴 \n",
    "        disc_real = disc(real).view(-1)\n",
    "        lossD_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "        disc_fake = disc(fake).view(-1)\n",
    "        lossD_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        lossD = (lossD_real+lossD_fake)/2\n",
    "        disc.zero_grad()\n",
    "        lossD.backward(retain_graph=True)\n",
    "        opt_disc.step()\n",
    "        \n",
    "        # output으로 니온 fake 이미지를 1에 맞출 수 있게 gen의 가중치를 맞춰줌 \n",
    "        output = disc(fake).view(-1)\n",
    "        lossG = criterion(output, torch.ones_like(output))\n",
    "        gen.zero_grad()\n",
    "        lossG.backward()\n",
    "        opt_gen.step()\n",
    "        if batch_idx == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(loader)} \\\n",
    "                      Loss D: {lossD:.4f}, loss G: {lossG:.4f}\"\n",
    "            )\n",
    "        with torch.no_grad():\n",
    "                fake = gen(fixed_noise).reshape(-1, 1, 28, 28)\n",
    "                data = real.reshape(-1, 1, 28, 28)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake, normalize=True)\n",
    "                img_grid_real = torchvision.utils.make_grid(data, normalize=True)\n",
    "\n",
    "                writer_fake.add_image(\n",
    "                    \"Mnist Fake Images2\", img_grid_fake, global_step=step\n",
    "                )\n",
    "                writer_real.add_image(\n",
    "                    \"Mnist Real Images2\", img_grid_real, global_step=step\n",
    "                )\n",
    "                step += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d811d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.7_imagetuto",
   "language": "python",
   "name": "3.7_imagetuto"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

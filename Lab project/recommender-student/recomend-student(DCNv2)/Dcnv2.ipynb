{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ade0f264-b24d-4217-8788-cd2408ac2d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dd9e9d8-fd7a-4cdc-9a9d-792b8cda517c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\db\\AppData\\Local\\Temp\\ipykernel_28680\\1374857115.py:2: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train= pd.read_csv(DATASET_DIR+ \"/NCF 졸업생 데이터 (1부).csv\", sep='|')[['sex','ccd','bzc_cd','grup_cd']]\n",
      "C:\\Users\\db\\AppData\\Local\\Temp\\ipykernel_28680\\1374857115.py:3: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  test = pd.read_csv(DATASET_DIR+ \"/NCF 졸업생 데이터 (2부).csv\", sep='|')[['sex','ccd','bzc_cd','grup_cd']]\n"
     ]
    }
   ],
   "source": [
    "DATASET_DIR = \"C:/Users/db/Desktop/기업-수강ncf/NCF 졸업생 데이터\"\n",
    "train= pd.read_csv(DATASET_DIR+ \"/NCF 졸업생 데이터 (1부).csv\", sep='|')[['sex','ccd','bzc_cd','grup_cd']]\n",
    "test = pd.read_csv(DATASET_DIR+ \"/NCF 졸업생 데이터 (2부).csv\", sep='|')[['sex','ccd','bzc_cd','grup_cd']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8696c966-d992-4454-a692-1aafa28a71e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_list = np.concatenate([train.values, test.values])\n",
    "n_rating_list = []\n",
    "for a in rating_list:\n",
    "    sex,ccd,bzc_cd,grup_cd = a\n",
    "    n_rating_list.append([sex,ccd,bzc_cd,grup_cd])\n",
    "rating_list = np.array(n_rating_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cca9616f-14c7-4ef0-8469-532fd4c781a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_ccd:314 num_bzc:925 num_item:16695\n"
     ]
    }
   ],
   "source": [
    "# 모든 유저와 모든 item을 담음\n",
    "unq_ccd, unq_bzccd,unq_grup_cd = [np.unique(rating_list[:, n+1]) for n in range(3)]\n",
    "# user와 item의 개수 넣음\n",
    "num_ccd, num_bzc, num_grup = len(unq_ccd), len(unq_bzccd),len(unq_grup_cd)\n",
    "print('num_ccd:{} num_bzc:{} num_item:{}'.format(num_ccd, num_bzc, num_grup))\n",
    "train_list = np.array(train)\n",
    "test_list = np.array(test)\n",
    "ccd2idx, bzcidx ,grupidx = {}, {},{}\n",
    "for i, j in enumerate(unq_ccd):\n",
    "     ccd2idx[j] = i\n",
    "for i, j in enumerate(unq_bzccd):\n",
    "     bzcidx[j] = i\n",
    "for i, j in enumerate(unq_grup_cd):\n",
    "     grupidx[j] = i    \n",
    "from sklearn.model_selection import train_test_split\n",
    "train_list = [[int(li[0])-1,ccd2idx[str(li[1])], bzcidx[str(li[2])],grupidx[str(li[3])]] for li in rating_list]\n",
    "# x_train, x_test = train_test_split(train_list, test_size=0.3, shuffle=True, random_state=34)\n",
    "train_list = pd.DataFrame(train_list, columns = ['sex', 'ccd','bzc','item'])\n",
    "x_train, x_test = train_test_split(train_list, test_size=0.3, shuffle=True, stratify =train_list[['sex', 'ccd','bzc']], random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6ac22b3-7715-4dc3-95ad-360731eff864",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DfData(torch.utils.data.Dataset):\n",
    "    def __init__(self,features,num_sex,num_ccd,num_bzc,num_grup,num_ng=0,train_mat=None,is_training=None):\n",
    "        super(DfData,self).__init__()\n",
    "        # self.features_ps = torch.Tensor(features).int()\n",
    "        self.features_ps = features\n",
    "        self.num_sex = num_sex\n",
    "        self.num_bzc = num_bzc\n",
    "        self.num_ccd = num_ccd\n",
    "        self.num_grup = num_grup\n",
    "        self.num_ng = num_ng\n",
    "        self.labels = [0] * len(features)\n",
    "        self.train_mat = train_mat\n",
    "        self.is_training = is_training\n",
    "        \n",
    "        # negative sampling \n",
    "    def set_ng_sample(self):\n",
    "        assert self.is_training, \"no need to sampling when testing\"\n",
    "         \n",
    "        self.features_ng = []\n",
    "        for x in self.features_ps:\n",
    "            # sex\n",
    "            for _ in range(self.num_ng):\n",
    "                s = np.random.randint(self.num_sex)\n",
    "                u = np.random.randint(self.num_ccd)\n",
    "                q = np.random.randint(self.num_bzc)\n",
    "                j = np.random.randint(self.num_grup)\n",
    "                # train set에 있는 경우 다시 뽑기\n",
    "                self.features_ng.append([s,u,q,j])\n",
    "\n",
    "        labels_ps = [1] * len(self.features_ps)\n",
    "        labels_ng = [0] * len(self.features_ng)\n",
    "        \n",
    "        self.features_fill = torch.Tensor(self.features_ps + self.features_ng).to(torch.int64)\n",
    "        self.labels_fill= torch.Tensor(labels_ps + labels_ng)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.num_ng+1) * len(self.labels)\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        features = self.features_fill if self.is_training else torch.Tensor(self.features_ps).to(torch.int64)\n",
    "        labels = self.labels_fill if self.is_training else torch.Tensor(self.labels)\n",
    "        \n",
    "        user = features[idx]\n",
    "        label = labels[idx]\n",
    "        return user, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "badd7e0e-1ca7-4a30-9281-0f65ea97fad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sex = 2\n",
    "x_train = x_train.values.tolist()\n",
    "x_test = x_test.values.tolist()\n",
    "train_dataset = DfData(x_train, num_sex,num_ccd,num_bzc, num_grup, num_ng=1, is_training=True)\n",
    "train_loader = data.DataLoader(train_dataset, batch_size = 256, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a952822-ad39-41f0-a820-ee56a0e20da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0,  259,   35, 6500],\n",
      "        [   1,  192,  494,  130],\n",
      "        [   0,   20,  753, 6831],\n",
      "        ...,\n",
      "        [   0,   95,  330, 4033],\n",
      "        [   1,  171,   26,  785],\n",
      "        [   1,   96,  347, 6997]])\n",
      "tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1.,\n",
      "        0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1.,\n",
      "        1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "        0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
      "        1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0.,\n",
      "        0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
      "        0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1.,\n",
      "        0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
      "        1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1.,\n",
      "        0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
      "        0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1.,\n",
      "        0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "train_dataset.set_ng_sample()\n",
    "for a,b in train_loader:\n",
    "    print(a)\n",
    "    print(b)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "418573e3-e726-4e4f-895e-f84763984efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesEmbedding(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, field_dims, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(sum(field_dims), embed_dim)\n",
    "        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.int64())\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Long tensor of size ``(batch_size, num_fields)``\n",
    "        \"\"\"\n",
    "        x = x + x.new_tensor(self.offsets).unsqueeze(0)\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bab4e2b-6414-47cf-8a28-ec5181beba11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesLinear(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, field_dims, output_dim=1):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Embedding(sum(field_dims), output_dim)\n",
    "        self.bias = torch.nn.Parameter(torch.zeros((output_dim,)))\n",
    "        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.int64())\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Long tensor of size ``(batch_size, num_fields)``\n",
    "        \"\"\"\n",
    "        x = x + x.new_tensor(self.offsets).unsqueeze(0)\n",
    "        return torch.sum(self.fc(x), dim=1) + self.bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "853ed5b6-7a46-41e1-b664-32c496611ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, embed_dims, dropout, output_layer=True):\n",
    "        super().__init__()\n",
    "        layers = list()\n",
    "        for embed_dim in embed_dims:\n",
    "            layers.append(torch.nn.Linear(input_dim, embed_dim))\n",
    "            layers.append(torch.nn.BatchNorm1d(embed_dim))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            layers.append(torch.nn.Dropout(p=dropout))\n",
    "            input_dim = embed_dim\n",
    "        if output_layer:\n",
    "            layers.append(torch.nn.Linear(input_dim, input_dim))\n",
    "        self.mlp = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Float tensor of size ``(batch_size, embed_dim)``\n",
    "        \"\"\"\n",
    "        return self.mlp(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60ce6fb4-1c4d-4338-ad56-ddbf2fe8ea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossNetwork(torch.nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(CrossNetwork, self).__init__()\n",
    "        self.weights = torch.nn.Parameter(torch.rand(input_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Cross network 연산: x * weights\n",
    "        return x * self.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cb68b89-594d-4069-9490-28c1a2b7d6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# class DcN(torch.nn.Module):\n",
    "#     \"\"\"\n",
    "#     A pytorch implementation of DeepFM.\n",
    "\n",
    "#     Reference:\n",
    "#         H Guo, et al. DeepFM: A Factorization-Machine based Neural Network for CTR Prediction, 2017.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, field_dims, embed_dim, mlp_dims, dropout):\n",
    "#         super().__init__()\n",
    "#         self.linear = FeaturesLinear(field_dims)\n",
    "#         self.embedding = FeaturesEmbedding(field_dims, embed_dim)\n",
    "#         self.embed_output_dim = len(field_dims) * embed_dim\n",
    "#         self.mlp = MultiLayerPerceptron(self.embed_output_dim, mlp_dims, dropout)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         :param x: Long tensor of size ``(batch_size, num_fields)``\n",
    "#         \"\"\"\n",
    "#         embed_x = self.embedding(x)\n",
    "#         x = self.linear(x) + self.mlp(embed_x.view(-1, self.embed_output_dim))\n",
    "#         # return torch.sigmoid(x.squeeze(1))\n",
    "#         return x.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7282d01c-aa23-471d-9e61-b1dcda99c143",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DcN(torch.nn.Module):\n",
    "    def __init__(self, field_dims, embed_dim, mlp_dims, dropout):\n",
    "        super(DcN, self).__init__()\n",
    "        # self.linear = FeaturesLinear(field_dims)\n",
    "        self.embedding = FeaturesEmbedding(field_dims, embed_dim)\n",
    "        self.embed_output_dim = len(field_dims) * embed_dim\n",
    "        self.cross_network = CrossNetwork(self.embed_output_dim)\n",
    "        self.mlp = MultiLayerPerceptron(self.embed_output_dim, mlp_dims, dropout)\n",
    "        # 출력 레이어\n",
    "        self.output_layer = torch.nn.Linear(self.embed_output_dim+ embed_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 선형 컴포넌트\n",
    "        # linear_output = self.linear(x)\n",
    "        # 임베딩 컴포넌트\n",
    "        embed_x = self.embedding(x)\n",
    "        # Cross Network 컴포넌트\n",
    "        cross_output = self.cross_network(embed_x.view(-1, self.embed_output_dim))\n",
    "        # MLP 컴포넌트\n",
    "        mlp_output = self.mlp(embed_x.view(-1, self.embed_output_dim))\n",
    "        # Cross network 출력과 MLP 출력을 연결\n",
    "        concat_output = torch.cat([cross_output, mlp_output], dim=1)\n",
    "        # 최종 출력 레이어\n",
    "        final_output = self.output_layer(concat_output)\n",
    "       \n",
    "        return final_output.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cde21594-43de-40bb-a7e0-843883b218ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sex = 2 \n",
    "field_dims = [sex, num_ccd,num_bzc,num_grup]\n",
    "embed_dim= 50\n",
    "mlp_dims = [50, 50] \n",
    "dropout= 0.2\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f926436d-5e86-4714-9c9a-d4325bd25a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DcN(\n",
       "  (embedding): FeaturesEmbedding(\n",
       "    (embedding): Embedding(17936, 50)\n",
       "  )\n",
       "  (cross_network): CrossNetwork()\n",
       "  (mlp): MultiLayerPerceptron(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=200, out_features=50, bias=True)\n",
       "      (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Dropout(p=0.2, inplace=False)\n",
       "      (4): Linear(in_features=50, out_features=50, bias=True)\n",
       "      (5): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU()\n",
       "      (7): Dropout(p=0.2, inplace=False)\n",
       "      (8): Linear(in_features=50, out_features=50, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(in_features=250, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = DcN(field_dims, embed_dim , mlp_dims, dropout)\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a51d89ed-30d9-48f1-919b-995800897025",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "epochs = 50\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c2b45e1-4034-43c5-b529-081395608cb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.176400155\n",
      "Epoch: 0002 cost = 0.077050246\n",
      "Epoch: 0003 cost = 0.055104233\n",
      "Epoch: 0004 cost = 0.044067748\n",
      "Epoch: 0005 cost = 0.037714224\n",
      "Epoch: 0006 cost = 0.033435464\n",
      "Epoch: 0007 cost = 0.029898429\n",
      "Epoch: 0008 cost = 0.027933672\n",
      "Epoch: 0009 cost = 0.025359726\n",
      "Epoch: 0010 cost = 0.023807231\n",
      "Epoch: 0011 cost = 0.022574542\n",
      "Epoch: 0012 cost = 0.021142099\n",
      "Epoch: 0013 cost = 0.020284399\n",
      "Epoch: 0014 cost = 0.019497331\n",
      "Epoch: 0015 cost = 0.018605445\n",
      "Epoch: 0016 cost = 0.017879575\n",
      "Epoch: 0017 cost = 0.017124616\n",
      "Epoch: 0018 cost = 0.016723346\n",
      "Epoch: 0019 cost = 0.016636958\n",
      "Epoch: 0020 cost = 0.016084675\n",
      "Epoch: 0021 cost = 0.015459342\n",
      "Epoch: 0022 cost = 0.015487282\n",
      "Epoch: 0023 cost = 0.014316119\n",
      "Epoch: 0024 cost = 0.014692402\n",
      "Epoch: 0025 cost = 0.014566072\n",
      "Epoch: 0026 cost = 0.013851991\n",
      "Epoch: 0027 cost = 0.014283016\n",
      "Epoch: 0028 cost = 0.013752769\n",
      "Epoch: 0029 cost = 0.013403537\n",
      "Epoch: 0030 cost = 0.013390922\n",
      "Epoch: 0031 cost = 0.013152560\n",
      "Epoch: 0032 cost = 0.012623364\n",
      "Epoch: 0033 cost = 0.013063401\n",
      "Epoch: 0034 cost = 0.012515304\n",
      "Epoch: 0035 cost = 0.012667711\n",
      "Epoch: 0036 cost = 0.012251113\n",
      "Epoch: 0037 cost = 0.012412335\n",
      "Epoch: 0038 cost = 0.012048789\n",
      "Epoch: 0039 cost = 0.012018317\n",
      "Epoch: 0040 cost = 0.012097384\n",
      "Epoch: 0041 cost = 0.011709495\n",
      "Epoch: 0042 cost = 0.011676516\n",
      "Epoch: 0043 cost = 0.011744831\n",
      "Epoch: 0044 cost = 0.011586554\n",
      "Epoch: 0045 cost = 0.011569098\n",
      "Epoch: 0046 cost = 0.011599329\n",
      "Epoch: 0047 cost = 0.011416041\n",
      "Epoch: 0048 cost = 0.011259915\n",
      "Epoch: 0049 cost = 0.011197947\n",
      "Epoch: 0050 cost = 0.011200835\n"
     ]
    }
   ],
   "source": [
    "for ep in range(epochs):   \n",
    "    avg_cost = 0\n",
    "    train_loader.dataset.set_ng_sample()\n",
    "    for user, label in train_loader:\n",
    "        user = user.to(torch.int64).to(device)\n",
    "        label = label.to('cuda:0')\n",
    "        net.zero_grad()\n",
    "        pred = net(user)\n",
    "        loss = loss_function(pred, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_cost += loss / len(train_loader)\n",
    "    net.eval()\n",
    "    # HR = metrics(net, test_loader, 10)    \n",
    "    # print(np.mean(HR))\n",
    "    print('Epoch:', '%04d' % (ep + 1), 'cost =', '{:.9f}'.format(avg_cost)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7558933-a372-4fa3-85ac-16dc0e881667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(net,'C:/Users/db/Desktop/DCN/dcn.pth')\n",
    "model_path = 'C:/Users/db/Desktop/DCN/dcn.pth'\n",
    "\n",
    "# 모델을 로드합니다.\n",
    "net = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fdeec4f-333b-4866-992b-934029860f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "341c9064-ce0b-48d7-90ec-5c392d6c06c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = {}\n",
    "for item in x_train:\n",
    "    key = tuple(item[:3])  # 2, 91, 750을 튜플로 만듭니다.\n",
    "    value = item[3]  # 나머지 값은 딕셔너리의 값으로 사용합니다.\n",
    "    \n",
    "    if key in train_dict:\n",
    "        train_dict[key].append(value)\n",
    "    else:\n",
    "        train_dict[key] = [value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4eb9a34-b87e-439c-995a-e48f2db88e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = {}\n",
    "for item in x_test:\n",
    "    key = tuple(item[:3])  # 2, 91, 750을 튜플로 만듭니다.\n",
    "    value = item[3]  # 나머지 값은 딕셔너리의 값으로 사용합니다.\n",
    "    \n",
    "    if key in result_dict:\n",
    "        result_dict[key].append(value)\n",
    "    else:\n",
    "        result_dict[key] = [value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a776d72e-59a8-4480-b708-16ce2d46ceec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[618, 6994, 12453, 509, 14047, 8935, 8942, 8956, 1732, 368, 8987, 8947, 642, 7004, 8963, 8940, 8987, 1076, 11162, 1408, 575, 8953, 8963, 8987, 6998, 8942, 1743, 8966, 1033, 8951, 8946, 1023, 8965, 7004, 8958, 575, 8939, 1016, 1076, 1019, 373, 8961, 8987, 8221, 8987, 380, 6994, 8951, 368, 367, 1668, 374, 8987, 1008, 373, 368, 8949, 8224, 8205, 8934, 7004, 8964, 8966, 1668, 998]\n"
     ]
    }
   ],
   "source": [
    "keys_list = list(result_dict.keys())\n",
    "for a in keys_list:\n",
    "    print(result_dict[a])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f168fd7-536d-4a3c-8ddc-abff36ee536c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Precision: 100%|███████████████████████████████████████████████████████| 9600/9600 [11:01<00:00, 14.52it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # Import tqdm for the progress bar\n",
    "\n",
    "def precision_and_map(k_values):\n",
    "    precision_results = {k: [] for k in k_values}\n",
    "    map_results = {k: [] for k in k_values}\n",
    "    recall_results = {k: [] for k in k_values}\n",
    "    \n",
    "    for i in tqdm(keys_list, desc=\"Calculating Precision\", position=0, leave=True):\n",
    "        p = []\n",
    "        try:\n",
    "            b = result_dict[i]\n",
    "            c = train_dict[i]\n",
    "        except KeyError:\n",
    "            continue\n",
    "        \n",
    "        for j in range(num_grup):\n",
    "            p.append(list(i) + [j])  # Fix variable name from 'a' to 'i'\n",
    "        \n",
    "        p = torch.tensor(p).to(device)\n",
    "        pred = net(p).to(torch.int)  # Change to 'torch.int' for data type\n",
    "        values, sorted_indices = torch.sort(pred, descending=True)\n",
    "        \n",
    "        original_indices = [idx for idx in sorted_indices.tolist() if idx not in set(c)]\n",
    "        \n",
    "        for k in k_values:\n",
    "            exclusive_indices = original_indices[:k]  # Use the original indices here\n",
    "            # Precision 계산\n",
    "            intersection_count_k = len(set(b).intersection(set(exclusive_indices)))\n",
    "            precision_results[k].append(intersection_count_k / k)\n",
    "            # recall 계산 \n",
    "            recall_results[k].append(intersection_count_k / len(b))\n",
    "            # MAP 계산\n",
    "            precision_at_i = []\n",
    "            \n",
    "            for rank, idx in enumerate(exclusive_indices, start=1):\n",
    "                if idx in set(b):\n",
    "                    precision_at_i.append(sum([1 if r in set(b) else 0 for r in exclusive_indices[:rank]]) / rank)\n",
    "            \n",
    "            if precision_at_i:\n",
    "                map_results[k].append(sum(precision_at_i) / len(precision_at_i))\n",
    "            else:\n",
    "                map_results[k].append(0.0)\n",
    "    return precision_results, map_results, recall_results\n",
    "\n",
    "k_values = [10, 25, 50, 100]\n",
    "precision_results, map_results, recall_results = precision_and_map(k_values)\n",
    "\n",
    "# Access the results\n",
    "precision_result_10 = precision_results[10]\n",
    "precision_result_25 = precision_results[25]\n",
    "precision_result_50 = precision_results[50]\n",
    "precision_result_100 = precision_results[100]\n",
    "\n",
    "map_result_10 = map_results[10]\n",
    "map_result_25 = map_results[25]\n",
    "map_result_50 = map_results[50]\n",
    "map_result_100 = map_results[100]\n",
    "\n",
    "recall_result_10 = recall_results[10]\n",
    "recall_result_25 = recall_results[25]\n",
    "recall_result_50 = recall_results[50]\n",
    "recall_result_100 = recall_results[100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52913dfe-6e0f-4eef-bc2e-4339497db839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@10 Mean: 0.1933125\n",
      "Precision@25 Mean: 0.15440416666666668\n",
      "Precision@50 Mean: 0.12157708333333335\n",
      "Precision@100 Mean: 0.08920729166666667\n",
      "Recall@10 Mean: 0.10411838123747362\n",
      "Recall@25 Mean: 0.20802839333683168\n",
      "Recall@50 Mean: 0.32587206164310706\n",
      "Recall@100 Mean: 0.47504792649663957\n",
      "MAP@10 Mean: 0.35573389520738846\n",
      "MAP@25 Mean: 0.3042322520368684\n",
      "MAP@50 Mean: 0.2586514440366133\n",
      "MAP@100 Mean: 0.21824454089997625\n"
     ]
    }
   ],
   "source": [
    "precision_10_mean = np.mean(precision_result_10)\n",
    "precision_25_mean = np.mean(precision_result_25)\n",
    "precision_50_mean = np.mean(precision_result_50)\n",
    "precision_100_mean = np.mean(precision_result_100)\n",
    "\n",
    "recall_10_mean = np.mean(recall_result_10)\n",
    "recall_25_mean = np.mean(recall_result_25)\n",
    "recall_50_mean = np.mean(recall_result_50)\n",
    "recall_100_mean = np.mean(recall_result_100)\n",
    "\n",
    "map_10_mean = np.mean(map_result_10)\n",
    "map_25_mean = np.mean(map_result_25)\n",
    "map_50_mean = np.mean(map_result_50)\n",
    "map_100_mean = np.mean(map_result_100)\n",
    "\n",
    "# 평균 출력\n",
    "print(\"Precision@10 Mean:\", precision_10_mean)\n",
    "print(\"Precision@25 Mean:\", precision_25_mean)\n",
    "print(\"Precision@50 Mean:\", precision_50_mean)\n",
    "print(\"Precision@100 Mean:\", precision_100_mean)\n",
    "\n",
    "print(\"Recall@10 Mean:\", recall_10_mean)\n",
    "print(\"Recall@25 Mean:\", recall_25_mean)\n",
    "print(\"Recall@50 Mean:\", recall_50_mean)\n",
    "print(\"Recall@100 Mean:\", recall_100_mean)\n",
    "\n",
    "print(\"MAP@10 Mean:\", map_10_mean)\n",
    "print(\"MAP@25 Mean:\", map_25_mean)\n",
    "print(\"MAP@50 Mean:\", map_50_mean)\n",
    "print(\"MAP@100 Mean:\", map_100_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1aa56cd9-6a1f-4579-ad40-3db52a2de94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm  # Import tqdm for the progress bar\n",
    "\n",
    "def precision(k):\n",
    "    result = []\n",
    "    for i in tqdm(keys_list, desc=\"Calculating Precision\", position=0, leave=True):  # tqdm setup\n",
    "        p = []\n",
    "        try:\n",
    "            b = result_dict[i]\n",
    "            c = train_dict[i]\n",
    "        except KeyError:\n",
    "            continue\n",
    "        for j in range(num_grup):\n",
    "            p.append(list(i) + [j])  # Fix variable name from 'a' to 'i'\n",
    "        \n",
    "        p = torch.tensor(p).to(device)\n",
    "        pred = net(p).to(torch.int)  # Change to 'torch.int' for data type\n",
    "        values, sorted_indices = torch.sort(pred, descending=True)\n",
    "        \n",
    "        exclusive_indices = [idx for idx in sorted_indices.tolist() if idx not in set(c)]\n",
    "        exclusive_indices = exclusive_indices[:k]\n",
    "        \n",
    "        intersection_count = len(set(b).intersection(set(exclusive_indices)))\n",
    "        result.append(intersection_count / k)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "def9de91-3968-4fe3-977a-a2ecf390757a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def calculate_ap(gt, pred, k):\n",
    "    \"\"\"\n",
    "    단일 클래스에 대한 평균 정밀도 (AP)를 계산합니다.\n",
    "    \"\"\"\n",
    "    precision_at_k = 0.0\n",
    "    true_positives = 0\n",
    "\n",
    "    for i in range(k):\n",
    "        if pred[i] in gt:\n",
    "            true_positives += 1\n",
    "            precision_at_k += true_positives / (i + 1)\n",
    "\n",
    "    if not gt:\n",
    "        return 0.0\n",
    "\n",
    "    return precision_at_k / len(gt)\n",
    "\n",
    "def mean_average_precision(k):\n",
    "    result = []\n",
    "\n",
    "    for i in tqdm(keys_list, desc=\"mAP 계산 중\", position=0, leave=True):\n",
    "        try:\n",
    "            b = result_dict[i]\n",
    "            c = train_dict[i]\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "        p = []\n",
    "        for j in range(num_grup):\n",
    "            p.append(list(i) + [j])\n",
    "        p = torch.tensor(p).to(device)\n",
    "        pred = net(p).to(torch.int)\n",
    "        values, sorted_indices = torch.sort(pred, descending=True)\n",
    "\n",
    "        exclusive_indices = [idx for idx in sorted_indices.tolist() if idx not in set(c)] \n",
    "        exclusive_indices = exclusive_indices[:k]\n",
    "\n",
    "        ap = calculate_ap(b, exclusive_indices, k)\n",
    "        result.append(ap)\n",
    "\n",
    "    mean_ap = sum(result) / len(result) if result else 0.0\n",
    "    return mean_ap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b667d955-3fd2-4e92-9e7e-234f99d267f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mAP 계산 중: 100%|█████████████████████████████████████████████████████████████████| 9600/9600 [06:14<00:00, 25.62it/s]\n",
      "mAP 계산 중: 100%|█████████████████████████████████████████████████████████████████| 9600/9600 [05:59<00:00, 26.67it/s]\n",
      "mAP 계산 중: 100%|█████████████████████████████████████████████████████████████████| 9600/9600 [06:03<00:00, 26.43it/s]\n",
      "mAP 계산 중: 100%|█████████████████████████████████████████████████████████████████| 9600/9600 [08:03<00:00, 19.87it/s]\n"
     ]
    }
   ],
   "source": [
    "map_10 = mean_average_precision(10)\n",
    "map_25 = mean_average_precision(25)\n",
    "map_50 = mean_average_precision(50)\n",
    "map_100 = mean_average_precision(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "269c54af-4071-4a0e-b28c-e10990d31799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05348804535284291\n",
      "0.07933083314491068\n",
      "0.09900535963675014\n",
      "0.11617460278865101\n"
     ]
    }
   ],
   "source": [
    "print(map_10)\n",
    "print(map_25)\n",
    "print(map_50)\n",
    "print(map_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "918187e1-d7fe-464a-a808-eec33689a1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Precision: 100%|███████████████████████████████████████████████████████| 9600/9600 [06:12<00:00, 25.75it/s]\n",
      "Calculating Precision: 100%|███████████████████████████████████████████████████████| 9600/9600 [06:12<00:00, 25.76it/s]\n",
      "Calculating Precision: 100%|███████████████████████████████████████████████████████| 9600/9600 [06:13<00:00, 25.73it/s]\n",
      "Calculating Precision: 100%|███████████████████████████████████████████████████████| 9600/9600 [09:53<00:00, 16.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "a_10 = precision(10)\n",
    "a_25 = precision(25)\n",
    "a_50 = precision(50)\n",
    "a_100 = precision(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "096d6d36-2f82-4a90-a0db-8b4b2bc0c579",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1933125\n",
      "0.15440416666666668\n",
      "0.12157708333333335\n",
      "0.08920729166666667\n"
     ]
    }
   ],
   "source": [
    "print(np.array(a_10).mean())\n",
    "print(np.array(a_25).mean())\n",
    "print(np.array(a_50).mean())\n",
    "print(np.array(a_100).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bdcc3f56-1407-4477-87ba-a7f5d26594be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm  # Import tqdm for the progress bar\n",
    "\n",
    "def Recall(k):\n",
    "    result = []\n",
    "    for i in tqdm(keys_list, desc=\"Calculating Precision\", position=0, leave=True):  # tqdm setup\n",
    "        p = []\n",
    "        try:\n",
    "            b = result_dict[i]\n",
    "            c = train_dict[i]\n",
    "        except KeyError:\n",
    "            continue\n",
    "        for j in range(num_grup):\n",
    "            p.append(list(i) + [j])  # Fix variable name from 'a' to 'i'\n",
    "        \n",
    "        p = torch.tensor(p).to(device)\n",
    "        pred = net(p).to(torch.int)  # Change to 'torch.int' for data type\n",
    "        values, sorted_indices = torch.sort(pred, descending=True)\n",
    "        \n",
    "        exclusive_indices = [idx for idx in sorted_indices.tolist() if idx not in set(c)]\n",
    "        exclusive_indices = exclusive_indices[:k]\n",
    "        \n",
    "        intersection_count = len(set(b).intersection(set(exclusive_indices)))\n",
    "        result.append(intersection_count / len(set(b)))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f6caf98-a27d-4886-aea3-9c2079a913de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Precision: 100%|███████████████████████████████████████████████████████| 9600/9600 [11:08<00:00, 14.35it/s]\n",
      "Calculating Precision: 100%|███████████████████████████████████████████████████████| 9600/9600 [10:35<00:00, 15.10it/s]\n",
      "Calculating Precision: 100%|███████████████████████████████████████████████████████| 9600/9600 [10:59<00:00, 14.55it/s]\n",
      "Calculating Precision: 100%|███████████████████████████████████████████████████████| 9600/9600 [10:56<00:00, 14.63it/s]\n"
     ]
    }
   ],
   "source": [
    "b_10 = Recall(10)\n",
    "b_25 = Recall(25)\n",
    "b_50 = Recall(50)\n",
    "b_100 = Recall(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69a09c69-1f1e-4974-bd85-7dd4041d05a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10769953555506812\n",
      "0.214977293497301\n",
      "0.33680489581424716\n",
      "0.49104891730649625\n"
     ]
    }
   ],
   "source": [
    "print(np.array(b_10).mean())\n",
    "print(np.array(b_25).mean())\n",
    "print(np.array(b_50).mean())\n",
    "print(np.array(b_100).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b666b9a-9e5d-469a-851e-0dc4c20afbf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn_3.9",
   "language": "python",
   "name": "gnn_3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

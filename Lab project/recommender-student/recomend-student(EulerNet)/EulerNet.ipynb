{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "333286ae-ac54-4a4d-935b-1eb0984cff5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd \n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39283229-2d43-4306-aae8-b85a193e7785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd \n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "772a212a-4ba3-454b-b84a-164fc9995471",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\db\\AppData\\Local\\Temp\\ipykernel_32072\\1374857115.py:2: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train= pd.read_csv(DATASET_DIR+ \"/NCF 졸업생 데이터 (1부).csv\", sep='|')[['sex','ccd','bzc_cd','grup_cd']]\n",
      "C:\\Users\\db\\AppData\\Local\\Temp\\ipykernel_32072\\1374857115.py:3: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  test = pd.read_csv(DATASET_DIR+ \"/NCF 졸업생 데이터 (2부).csv\", sep='|')[['sex','ccd','bzc_cd','grup_cd']]\n"
     ]
    }
   ],
   "source": [
    "DATASET_DIR = \"C:/Users/db/Desktop/기업-수강ncf/NCF 졸업생 데이터\"\n",
    "train= pd.read_csv(DATASET_DIR+ \"/NCF 졸업생 데이터 (1부).csv\", sep='|')[['sex','ccd','bzc_cd','grup_cd']]\n",
    "test = pd.read_csv(DATASET_DIR+ \"/NCF 졸업생 데이터 (2부).csv\", sep='|')[['sex','ccd','bzc_cd','grup_cd']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49b84065-7eb2-4638-b64a-af548207e7bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stid</th>\n",
       "      <th>sex</th>\n",
       "      <th>ccd</th>\n",
       "      <th>rk</th>\n",
       "      <th>yy</th>\n",
       "      <th>hk</th>\n",
       "      <th>grup_cd</th>\n",
       "      <th>s_avg</th>\n",
       "      <th>hcd</th>\n",
       "      <th>hired_yy</th>\n",
       "      <th>co</th>\n",
       "      <th>bzc_cd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201313917</td>\n",
       "      <td>1</td>\n",
       "      <td>240301252</td>\n",
       "      <td>7</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>4480022</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>이천시청 정구선수단</td>\n",
       "      <td>84113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201313917</td>\n",
       "      <td>1</td>\n",
       "      <td>240301252</td>\n",
       "      <td>7</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>4480058</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>이천시청 정구선수단</td>\n",
       "      <td>84113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201313917</td>\n",
       "      <td>1</td>\n",
       "      <td>240301252</td>\n",
       "      <td>7</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>4480993</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>이천시청 정구선수단</td>\n",
       "      <td>84113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201313917</td>\n",
       "      <td>1</td>\n",
       "      <td>240301252</td>\n",
       "      <td>7</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>4482010</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>이천시청 정구선수단</td>\n",
       "      <td>84113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201314021</td>\n",
       "      <td>1</td>\n",
       "      <td>1150954000</td>\n",
       "      <td>7</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1200126</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>(주)에스디</td>\n",
       "      <td>42311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709976</th>\n",
       "      <td>201314030</td>\n",
       "      <td>2</td>\n",
       "      <td>1150954000</td>\n",
       "      <td>7</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>4393014</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "      <td>종근당건강(주)</td>\n",
       "      <td>10797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709977</th>\n",
       "      <td>201314030</td>\n",
       "      <td>2</td>\n",
       "      <td>1150954000</td>\n",
       "      <td>7</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>4393035</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "      <td>종근당건강(주)</td>\n",
       "      <td>10797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709978</th>\n",
       "      <td>201314030</td>\n",
       "      <td>2</td>\n",
       "      <td>1150954000</td>\n",
       "      <td>7</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>4393036</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "      <td>종근당건강(주)</td>\n",
       "      <td>10797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709979</th>\n",
       "      <td>201314030</td>\n",
       "      <td>2</td>\n",
       "      <td>1150954000</td>\n",
       "      <td>7</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>4393039</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "      <td>종근당건강(주)</td>\n",
       "      <td>10797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709980</th>\n",
       "      <td>201314030</td>\n",
       "      <td>2</td>\n",
       "      <td>1150954000</td>\n",
       "      <td>7</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>4393040</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "      <td>종근당건강(주)</td>\n",
       "      <td>10797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>709981 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             stid  sex         ccd  rk    yy  hk  grup_cd  s_avg  hcd  \\\n",
       "0       201313917    1   240301252   7  2015   1  4480022    2.5    1   \n",
       "1       201313917    1   240301252   7  2015   1  4480058    4.5    1   \n",
       "2       201313917    1   240301252   7  2015   1  4480993    0.0    1   \n",
       "3       201313917    1   240301252   7  2015   1  4482010    4.5    1   \n",
       "4       201314021    1  1150954000   7  2015   1  1200126    3.5    1   \n",
       "...           ...  ...         ...  ..   ...  ..      ...    ...  ...   \n",
       "709976  201314030    2  1150954000   7  2015   1  4393014    4.0    1   \n",
       "709977  201314030    2  1150954000   7  2015   1  4393035    2.5    1   \n",
       "709978  201314030    2  1150954000   7  2015   1  4393036    3.0    1   \n",
       "709979  201314030    2  1150954000   7  2015   1  4393039    2.5    1   \n",
       "709980  201314030    2  1150954000   7  2015   1  4393040    2.5    1   \n",
       "\n",
       "        hired_yy          co  bzc_cd  \n",
       "0           2017  이천시청 정구선수단   84113  \n",
       "1           2017  이천시청 정구선수단   84113  \n",
       "2           2017  이천시청 정구선수단   84113  \n",
       "3           2017  이천시청 정구선수단   84113  \n",
       "4           2017      (주)에스디   42311  \n",
       "...          ...         ...     ...  \n",
       "709976      2018    종근당건강(주)   10797  \n",
       "709977      2018    종근당건강(주)   10797  \n",
       "709978      2018    종근당건강(주)   10797  \n",
       "709979      2018    종근당건강(주)   10797  \n",
       "709980      2018    종근당건강(주)   10797  \n",
       "\n",
       "[709981 rows x 12 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a315c76-f127-4ba4-b9e9-47b5dfb21d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_list = np.concatenate([train.values, test.values])\n",
    "n_rating_list = []\n",
    "for a in rating_list:\n",
    "    sex,ccd,bzc_cd,grup_cd = a\n",
    "    n_rating_list.append([sex,ccd,bzc_cd,grup_cd])\n",
    "rating_list = np.array(n_rating_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6436eb90-af88-4101-9e9a-c909654ccedf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['1', '240301252', '84113', '4480022'],\n",
       "       ['1', '240301252', '84113', '4480058'],\n",
       "       ['1', '240301252', '84113', '4480993'],\n",
       "       ...,\n",
       "       ['2', '1150954000', '21102', '1300012'],\n",
       "       ['2', '1150954000', '21102', '4393039'],\n",
       "       ['2', '1150954000', '21102', '4393993']], dtype='<U11')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9c3f901-9070-48c5-9205-a3b331a1590d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_ccd:314 num_bzc:925 num_item:16695\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# 모든 유저와 모든 item을 담음\n",
    "unq_ccd, unq_bzccd,unq_grup_cd = [np.unique(rating_list[:, n+1]) for n in range(3)]\n",
    "# user와 item의 개수 넣음\n",
    "num_ccd, num_bzc, num_grup = len(unq_ccd), len(unq_bzccd),len(unq_grup_cd)\n",
    "print('num_ccd:{} num_bzc:{} num_item:{}'.format(num_ccd, num_bzc, num_grup))\n",
    "train_list = np.array(train)\n",
    "test_list = np.array(test)\n",
    "ccd2idx, bzcidx ,grupidx = {}, {},{}\n",
    "for i, j in enumerate(unq_ccd):\n",
    "     ccd2idx[j] = i\n",
    "for i, j in enumerate(unq_bzccd):\n",
    "     bzcidx[j] = i\n",
    "for i, j in enumerate(unq_grup_cd):\n",
    "     grupidx[j] = i    \n",
    "train_list = [[int(li[0])-1,ccd2idx[str(li[1])], bzcidx[str(li[2])],grupidx[str(li[3])]] for li in rating_list]\n",
    "# x_train, x_test = train_test_split(train_list, test_size=0.3, shuffle=True, random_state=34)\n",
    "train_list = pd.DataFrame(train_list, columns = ['sex', 'ccd','bzc','item'])\n",
    "x_train, x_test = train_test_split(train_list, test_size=0.3, shuffle=True, stratify =train_list[['sex', 'ccd','bzc']], random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe03f1b4-57e0-4689-b43f-47b0ba7ee5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DfData(torch.utils.data.Dataset):\n",
    "    def __init__(self,features,num_sex,num_ccd,num_bzc,num_grup,num_ng=0,train_mat=None,is_training=None):\n",
    "        super(DfData,self).__init__()\n",
    "        # self.features_ps = torch.Tensor(features).int()\n",
    "        self.features_ps = features\n",
    "        self.num_sex = num_sex\n",
    "        self.num_bzc = num_bzc\n",
    "        self.num_ccd = num_ccd\n",
    "        self.num_grup = num_grup\n",
    "        self.num_ng = num_ng\n",
    "        self.labels = [1] * len(features)\n",
    "        self.train_mat = train_mat\n",
    "        self.is_training = is_training\n",
    "        \n",
    "        # negative sampling \n",
    "    def set_ng_sample(self):\n",
    "        assert self.is_training, \"no need to sampling when testing\"\n",
    "         \n",
    "        self.features_ng = []\n",
    "        for x in self.features_ps:\n",
    "            # sex\n",
    "            for _ in range(self.num_ng):\n",
    "                s = np.random.randint(self.num_sex)\n",
    "                u = np.random.randint(self.num_ccd)\n",
    "                q = np.random.randint(self.num_bzc)\n",
    "                j = np.random.randint(self.num_grup)\n",
    "                # train set에 있는 경우 다시 뽑기\n",
    "                self.features_ng.append([s,u,q,j])\n",
    "\n",
    "        labels_ps = [1] * len(self.features_ps)\n",
    "        labels_ng = [0] * len(self.features_ng)\n",
    "        \n",
    "        self.features_fill = torch.Tensor(self.features_ps + self.features_ng).to(torch.int64)\n",
    "        self.labels_fill= torch.Tensor(labels_ps + labels_ng)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.num_ng+1) * len(self.labels)\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        features = self.features_fill if self.is_training else torch.Tensor(self.features_ps).to(torch.int64)\n",
    "        labels = self.labels_fill if self.is_training else torch.Tensor(self.labels)\n",
    "        user = features[idx]\n",
    "        label = labels[idx]\n",
    "        return user, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e76ed56-b9b8-43fa-869b-30f7d31f7696",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sex = 2\n",
    "x_train = x_train.values.tolist()\n",
    "x_test = x_test.values.tolist()\n",
    "train_dataset = DfData(x_train, num_sex,num_ccd,num_bzc, num_grup, num_ng=1, is_training=True)\n",
    "test_dataset = DfData(x_test, num_sex,num_ccd,num_bzc, num_grup)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size = 128, shuffle=False, num_workers=0)\n",
    "train_loader = data.DataLoader(train_dataset, batch_size = 128, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0eca7465-727d-4206-be46-701a868b5300",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = DfData(x_test, num_sex,num_ccd,num_bzc, num_grup, num_ng=1, is_training=True)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size = 128, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0252bd0-15df-46d9-a0ec-9e6385a13520",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesEmbedding(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, field_dims, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(sum(field_dims), embed_dim)\n",
    "        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.int64())\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Long tensor of size ``(batch_size, num_fields)``\n",
    "        \"\"\"\n",
    "        x = x + x.new_tensor(self.offsets).unsqueeze(0)\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e7fce85-28bd-4a84-8737-29685f9d8fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EulerInteractionLayer(nn.Module):\n",
    "    def __init__(self,  inshape, outshape, embed_dim):\n",
    "        super().__init__()\n",
    "        self.inshape, self.outshape = inshape, outshape\n",
    "        self.feature_dim = embed_dim\n",
    "\n",
    "\n",
    "        # Initial assignment of the order vectors, which significantly affects the training effectiveness of the model.\n",
    "        # We empirically provide two effective initialization methods here.\n",
    "        # How to better initialize is still a topic to be further explored.\n",
    "        # Note: 👆 👆 👆\n",
    "        if inshape == outshape:\n",
    "            init_orders = torch.eye(inshape // self.feature_dim, outshape // self.feature_dim)\n",
    "        else:\n",
    "            init_orders = torch.softmax(torch.randn(inshape // self.feature_dim, outshape // self.feature_dim) / 0.01, dim = 0)\n",
    "        \n",
    "        self.inter_orders = nn.Parameter(init_orders)\n",
    "        self.im = nn.Linear(inshape, outshape)\n",
    "        nn.init.normal_(self.im.weight , mean = 0 , std = 0.01)\n",
    "\n",
    "        self.bias_lam = nn.Parameter(torch.randn(1, self.feature_dim, outshape // self.feature_dim) * 0.01)\n",
    "        self.bias_theta = nn.Parameter(torch.randn(1, self.feature_dim, outshape // self.feature_dim) * 0.01)\n",
    "\n",
    "        self.drop_ex = nn.Dropout(0.3)\n",
    "        self.drop_im = nn.Dropout(p = 0.3)\n",
    "        self.norm_r = nn.LayerNorm([self.feature_dim])\n",
    "        self.norm_p = nn.LayerNorm([self.feature_dim])\n",
    "        \n",
    "\n",
    "    def forward(self, complex_features):\n",
    "        r, p = complex_features\n",
    "\n",
    "        lam = r ** 2 + p ** 2 + 1e-8\n",
    "        theta = torch.atan2(p, r)\n",
    "        lam, theta = lam.reshape(lam.shape[0], -1, self.feature_dim), theta.reshape(theta.shape[0], -1, self.feature_dim)\n",
    "        lam = 0.5 * torch.log(lam)\n",
    "        lam, theta = torch.transpose(lam, -2, -1), torch.transpose(theta, -2, -1)\n",
    "        lam, theta = self.drop_ex(lam), self.drop_ex(theta)\n",
    "        lam, theta =  lam @ (self.inter_orders) + self.bias_lam,  theta @ (self.inter_orders) + self.bias_theta\n",
    "        lam = torch.exp(lam)\n",
    "        lam, theta = torch.transpose(lam, -2, -1), torch.transpose(theta, -2, -1)\n",
    "\n",
    "        r, p = r.reshape(r.shape[0], -1), p.reshape(p.shape[0], -1)\n",
    "        r, p = self.drop_im(r), self.drop_im(p)\n",
    "        r, p = self.im(r), self.im(p)\n",
    "        r, p = torch.relu(r), torch.relu(p)\n",
    "        r, p = r.reshape(r.shape[0], -1, self.feature_dim), p.reshape(p.shape[0], -1, self.feature_dim)\n",
    "        \n",
    "        o_r, o_p =  r + lam * torch.cos(theta), p + lam * torch.sin(theta)\n",
    "        o_r, o_p = o_r.reshape(o_r.shape[0], -1, self.feature_dim), o_p.reshape(o_p.shape[0], -1, self.feature_dim)\n",
    "        # if self.apply_norm:\n",
    "        #     o_r, o_p = self.norm_r(o_r), self.norm_p(o_p)\n",
    "        return o_r, o_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "856bd247-e31f-447c-8452-271e0497178f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EulerNet(torch.nn.Module):\n",
    "    def __init__(self, field_dims, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = FeaturesEmbedding(field_dims, embed_dim)\n",
    "        field_num = len(field_dims) \n",
    "        order_list = [field_num]\n",
    "        shape_list = [embed_dim * field_num] + [num_neurons * embed_dim for num_neurons in order_list]\n",
    "\n",
    "        interaction_shapes = []\n",
    "        for inshape, outshape in zip(shape_list[:-1], shape_list[1:]):\n",
    "            interaction_shapes.append(EulerInteractionLayer(inshape, outshape,embed_dim))\n",
    "\n",
    "        self.Euler_interaction_layers = nn.Sequential(*interaction_shapes)\n",
    "        self.mu = nn.Parameter(torch.ones(1, field_num, 1))\n",
    "        self.reg = nn.Linear(shape_list[-1], 1)\n",
    "        nn.init.xavier_normal_(self.reg.weight)\n",
    "\n",
    "    def forward(self, feature):\n",
    "        feature = self.embedding(feature)\n",
    "        r, p = self.mu * torch.cos(feature), self.mu * torch.sin(feature)\n",
    "        o_r, o_p = self.Euler_interaction_layers((r, p))\n",
    "        o_r, o_p = o_r.reshape(o_r.shape[0], -1), o_p.reshape(o_p.shape[0], -1)\n",
    "        re, im = self.reg(o_r), self.reg(o_p)\n",
    "        self.logits = re + im\n",
    "        self.output = self.logits.squeeze(1)\n",
    "        return self.output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987e2913-adac-4ce7-8418-9ed7c2a9916f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fbbaccbe-fb72-4786-b9e2-40dc5732d906",
   "metadata": {},
   "outputs": [],
   "source": [
    "sex = 2 \n",
    "field_dims = [sex, num_ccd,num_bzc,num_grup]\n",
    "embed_dim= 50\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1d96a17-1412-49f0-bc90-7fef7be96aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "net =  EulerNet(field_dims , embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13f66e7e-f95e-4bb4-81de-de81a46c9180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EulerNet(\n",
       "  (embedding): FeaturesEmbedding(\n",
       "    (embedding): Embedding(17936, 50)\n",
       "  )\n",
       "  (Euler_interaction_layers): Sequential(\n",
       "    (0): EulerInteractionLayer(\n",
       "      (im): Linear(in_features=200, out_features=200, bias=True)\n",
       "      (drop_ex): Dropout(p=0.3, inplace=False)\n",
       "      (drop_im): Dropout(p=0.3, inplace=False)\n",
       "      (norm_r): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_p): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (reg): Linear(in_features=200, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30a61a36-4700-4e9c-b9bc-f76b5d80ae8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "epochs = 50\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad9b9798-a543-4450-8e9a-83828224f308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.183817670\n",
      "Epoch: 0002 cost = 0.079450287\n",
      "Epoch: 0003 cost = 0.054364301\n",
      "Epoch: 0004 cost = 0.042966928\n",
      "Epoch: 0005 cost = 0.035959858\n",
      "Epoch: 0006 cost = 0.031221243\n",
      "Epoch: 0007 cost = 0.027339574\n",
      "Epoch: 0008 cost = 0.024981735\n",
      "Epoch: 0009 cost = 0.022988668\n",
      "Epoch: 0010 cost = 0.020968203\n",
      "Epoch: 0011 cost = 0.019300494\n",
      "Epoch: 0012 cost = 0.018501198\n",
      "Epoch: 0013 cost = 0.017513894\n",
      "Epoch: 0014 cost = 0.016814334\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_function(pred, label)\n\u001b[0;32m     10\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 11\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     avg_cost \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[0;32m     13\u001b[0m net\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32m~\\.conda\\envs\\gnn_3.9\\lib\\site-packages\\torch\\optim\\optimizer.py:109\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    107\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\gnn_3.9\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\gnn_3.9\\lib\\site-packages\\torch\\optim\\adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    153\u001b[0m                 max_exp_avg_sqs\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_exp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    155\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 157\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m         \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m         \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m         \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m         \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m         \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m         \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\.conda\\envs\\gnn_3.9\\lib\\site-packages\\torch\\optim\\adam.py:213\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 213\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\gnn_3.9\\lib\\site-packages\\torch\\optim\\adam.py:265\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m    264\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mmul_(beta1)\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[1;32m--> 265\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable:\n\u001b[0;32m    268\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for ep in range(epochs):   \n",
    "    avg_cost = 0\n",
    "    train_loader.dataset.set_ng_sample()\n",
    "    for user, label in train_loader:\n",
    "        user = user.to(torch.int64).to(device)\n",
    "        label = label.to('cuda:0')\n",
    "        net.zero_grad()\n",
    "        pred = net(user)\n",
    "        loss = loss_function(pred, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_cost += loss / len(train_loader)\n",
    "    net.eval()\n",
    "    # HR = metrics(net, test_loader, 10)    \n",
    "    # print(np.mean(HR))\n",
    "    print('Epoch:', '%04d' % (ep + 1), 'cost =', '{:.9f}'.format(avg_cost)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c2da325-84e3-4468-8b3c-e8e6d3513210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = \"C:/Users/db/Desktop/eurlent/eurlent.pth\"\n",
    "\n",
    "# # 모델을 로드합니다.\n",
    "# net = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c9c8e6b-3950-4b9e-bc76-8d0348ccd404",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "16816576-d24e-41d8-ac1b-dc29dc287b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = {}\n",
    "for item in x_train:\n",
    "    key = tuple(item[:3])  # 2, 91, 750을 튜플로 만듭니다.\n",
    "    value = item[3]  # 나머지 값은 딕셔너리의 값으로 사용합니다.\n",
    "    \n",
    "    if key in train_dict:\n",
    "        train_dict[key].append(value)\n",
    "    else:\n",
    "        train_dict[key] = [value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "40c29f52-ae38-4a58-a138-f0b71e678e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = {}\n",
    "for item in x_test:\n",
    "    key = tuple(item[:3])  # 2, 91, 750을 튜플로 만듭니다.\n",
    "    value = item[3]  # 나머지 값은 딕셔너리의 값으로 사용합니다.\n",
    "    \n",
    "    if key in result_dict:\n",
    "        result_dict[key].append(value)\n",
    "    else:\n",
    "        result_dict[key] = [value]\n",
    "keys_list = list(train_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e797fd8-eba7-4dab-9f37-b23ae17db7a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.749895833333333"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test 셋 비율확인 \n",
    "dic2 = []\n",
    "for i in result_dict.keys():\n",
    "    dic2.append(len(result_dict[i]))\n",
    "# 평균적인 과목의 개수 \n",
    "(sum(dic2))/len(dic2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "07cda66c-edee-43a2-8332-ee4802835ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.7496875"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic = []\n",
    "for i in train_dict.keys():\n",
    "    dic.append(len(train_dict[i]))\n",
    "# 평균적인 과목의 개수 \n",
    "(sum(dic))/len(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "03197fcb-af27-44e5-bb0c-85a9eb98246c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def calculate_ap(gt, pred, k):\n",
    "    \"\"\"\n",
    "    단일 클래스에 대한 평균 정밀도 (AP)를 계산합니다.\n",
    "    \"\"\"\n",
    "    precision_at_k = 0.0\n",
    "    true_positives = 0\n",
    "\n",
    "    for i in range(k):\n",
    "        if pred[i] in gt:\n",
    "            true_positives += 1\n",
    "            precision_at_k += true_positives / (i + 1)\n",
    "\n",
    "    if not gt:\n",
    "        return 0.0\n",
    "\n",
    "    return precision_at_k / len(gt)\n",
    "\n",
    "def mean_average_precision(k):\n",
    "    result = []\n",
    "\n",
    "    for i in tqdm(keys_list, desc=\"mAP 계산 중\", position=0, leave=True):\n",
    "        try:\n",
    "            b = result_dict[i]\n",
    "            c = train_dict[i]\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "        p = []\n",
    "        for j in range(num_grup):\n",
    "            p.append(list(i) + [j])\n",
    "        p = torch.tensor(p).to(device)\n",
    "        pred = net(p).to(torch.int)\n",
    "        values, sorted_indices = torch.sort(pred, descending=True)\n",
    "\n",
    "        exclusive_indices = [idx for idx in sorted_indices.tolist() if idx not in set(c)] \n",
    "        exclusive_indices = exclusive_indices[:k]\n",
    "\n",
    "        ap = calculate_ap(b, exclusive_indices, k)\n",
    "        result.append(ap)\n",
    "\n",
    "    mean_ap = sum(result) / len(result) if result else 0.0\n",
    "    return mean_ap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d8a9909-c2b7-451b-8398-6be3a2c8dcb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mAP 계산 중: 100%|█████████████████████████████████████████████████████████████████| 9600/9600 [12:29<00:00, 12.82it/s]\n",
      "mAP 계산 중: 100%|█████████████████████████████████████████████████████████████████| 9600/9600 [11:26<00:00, 13.98it/s]\n",
      "mAP 계산 중: 100%|█████████████████████████████████████████████████████████████████| 9600/9600 [11:16<00:00, 14.19it/s]\n",
      "mAP 계산 중: 100%|█████████████████████████████████████████████████████████████████| 9600/9600 [11:17<00:00, 14.17it/s]\n"
     ]
    }
   ],
   "source": [
    "map_10 = mean_average_precision(10)\n",
    "map_25 = mean_average_precision(25)\n",
    "map_50 = mean_average_precision(50)\n",
    "map_100 = mean_average_precision(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e745d360-791b-4f29-b001-5212218e174c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05588498331850875\n",
      "0.08214926542347363\n",
      "0.10114489021418158\n",
      "0.11784293368635597\n"
     ]
    }
   ],
   "source": [
    "print(map_10)\n",
    "print(map_25)\n",
    "print(map_50)\n",
    "print(map_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee67168-45f7-4613-a07c-031423634f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm  # Import tqdm for the progress bar\n",
    "\n",
    "def precision(k):\n",
    "    result = []\n",
    "    for i in tqdm(keys_list, desc=\"Calculating Precision\", position=0, leave=True):  # tqdm setup\n",
    "        p = []\n",
    "        try:\n",
    "            b = result_dict[i]\n",
    "            c = train_dict[i]\n",
    "        except KeyError:\n",
    "            continue\n",
    "        for j in range(num_grup):\n",
    "            p.append(list(i) + [j])  # Fix variable name from 'a' to 'i'\n",
    "        p = torch.tensor(p).to(device)\n",
    "        pred = net(p).to(torch.int)  # Change to 'torch.int' for data type\n",
    "        values, sorted_indices = torch.sort(pred, descending=True)\n",
    "        \n",
    "        exclusive_indices = [idx for idx in sorted_indices.tolist() if idx not in set(c)] \n",
    "        exclusive_indices = exclusive_indices[:k]\n",
    "        intersection_count = len(set(b).intersection(set(exclusive_indices)))\n",
    "        result.append(intersection_count / k)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "95463e6f-846d-46d1-9f46-19a472cad736",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Precision: 100%|█████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1936, 1944, 1952, 1992,   69,  282, 1846, 1940, 1947, 1951, 1956, 1957,\n",
      "        1960, 1962, 1963, 1964, 1965, 1969, 1970, 1972, 1975, 1976, 1981, 1984,\n",
      "        1985, 1986, 1987, 1988, 1993, 1997], device='cuda:0')\n",
      "[1936, 69, 1846, 1969, 1972, 1986, 1988, 1993, 1997, 51]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(keys_list[:1], desc=\"Calculating Precision\", position=0, leave=True):\n",
    "    p = []\n",
    "    try:\n",
    "        b = result_dict[i]\n",
    "        c = train_dict[i]\n",
    "    except KeyError:\n",
    "        continue\n",
    "    \n",
    "    for j in range(num_grup):\n",
    "        p.append(list(i) + [j])  # Fix variable name from 'a' to 'i'\n",
    "    \n",
    "    p = torch.tensor(p).to(device)\n",
    "    pred = net(p).to(torch.int)  # Change to 'torch.int' for data type\n",
    "    values, sorted_indices = torch.sort(pred, descending=True)\n",
    "    print(sorted_indices[:30])\n",
    "    original_indices = [idx for idx in sorted_indices.tolist() if idx not in set(c)]\n",
    "    print(original_indices[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "28e42c7f-1102-4580-b059-b070d00dd0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Precision: 100%|███████████████████████████████████████████████████████| 9600/9600 [12:54<00:00, 12.39it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # Import tqdm for the progress bar\n",
    "\n",
    "def precision_and_map(k_values):\n",
    "    precision_results = {k: [] for k in k_values}\n",
    "    map_results = {k: [] for k in k_values}\n",
    "    recall_results = {k: [] for k in k_values}\n",
    "    \n",
    "    for i in tqdm(keys_list, desc=\"Calculating Precision\", position=0, leave=True):\n",
    "        p = []\n",
    "        try:\n",
    "            b = result_dict[i]\n",
    "            c = train_dict[i]\n",
    "        except KeyError:\n",
    "            continue\n",
    "        \n",
    "        for j in range(num_grup):\n",
    "            p.append(list(i) + [j])  # Fix variable name from 'a' to 'i'\n",
    "        \n",
    "        p = torch.tensor(p).to(device)\n",
    "        pred = net(p).to(torch.int)  # Change to 'torch.int' for data type\n",
    "        values, sorted_indices = torch.sort(pred, descending=True)\n",
    "        \n",
    "        original_indices = [idx for idx in sorted_indices.tolist() if idx not in set(c)]\n",
    "        \n",
    "        for k in k_values:\n",
    "            exclusive_indices = original_indices[:k]  # Use the original indices here\n",
    "            # Precision 계산\n",
    "            intersection_count_k = len(set(b).intersection(set(exclusive_indices)))\n",
    "            precision_results[k].append(intersection_count_k / k)\n",
    "            # recall 계산 \n",
    "            recall_results[k].append(intersection_count_k / len(b))\n",
    "            # MAP 계산\n",
    "            precision_at_i = []\n",
    "            \n",
    "            for rank, idx in enumerate(exclusive_indices, start=1):\n",
    "                if idx in set(b):\n",
    "                    precision_at_i.append(sum([1 if r in set(b) else 0 for r in exclusive_indices[:rank]]) / rank)\n",
    "            \n",
    "            if precision_at_i:\n",
    "                map_results[k].append(sum(precision_at_i) / len(precision_at_i))\n",
    "            else:\n",
    "                map_results[k].append(0.0)\n",
    "    return precision_results, map_results, recall_results\n",
    "\n",
    "k_values = [10, 25, 50, 100]\n",
    "precision_results, map_results, recall_results = precision_and_map(k_values)\n",
    "\n",
    "# Access the results\n",
    "precision_result_10 = precision_results[10]\n",
    "precision_result_25 = precision_results[25]\n",
    "precision_result_50 = precision_results[50]\n",
    "precision_result_100 = precision_results[100]\n",
    "\n",
    "map_result_10 = map_results[10]\n",
    "map_result_25 = map_results[25]\n",
    "map_result_50 = map_results[50]\n",
    "map_result_100 = map_results[100]\n",
    "\n",
    "recall_result_10 = recall_results[10]\n",
    "recall_result_25 = recall_results[25]\n",
    "recall_result_50 = recall_results[50]\n",
    "recall_result_100 = recall_results[100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4c324cd2-a530-4f64-a077-1900ce7bdd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@10 Mean: 0.198125\n",
      "Precision@25 Mean: 0.15697083333333336\n",
      "Precision@50 Mean: 0.1217916666666667\n",
      "Precision@100 Mean: 0.08877291666666667\n",
      "Recall@10 Mean: 0.10721837772174389\n",
      "Recall@25 Mean: 0.21233277465106365\n",
      "Recall@50 Mean: 0.3271400957263754\n",
      "Recall@100 Mean: 0.4728926174633057\n",
      "MAP@10 Mean: 0.3632200193426136\n",
      "MAP@25 Mean: 0.3096626697258589\n",
      "MAP@50 Mean: 0.2635194162559319\n",
      "MAP@100 Mean: 0.2232019756896978\n"
     ]
    }
   ],
   "source": [
    "precision_10_mean = np.mean(precision_result_10)\n",
    "precision_25_mean = np.mean(precision_result_25)\n",
    "precision_50_mean = np.mean(precision_result_50)\n",
    "precision_100_mean = np.mean(precision_result_100)\n",
    "\n",
    "recall_10_mean = np.mean(recall_result_10)\n",
    "recall_25_mean = np.mean(recall_result_25)\n",
    "recall_50_mean = np.mean(recall_result_50)\n",
    "recall_100_mean = np.mean(recall_result_100)\n",
    "\n",
    "map_10_mean = np.mean(map_result_10)\n",
    "map_25_mean = np.mean(map_result_25)\n",
    "map_50_mean = np.mean(map_result_50)\n",
    "map_100_mean = np.mean(map_result_100)\n",
    "\n",
    "# 평균 출력\n",
    "print(\"Precision@10 Mean:\", precision_10_mean)\n",
    "print(\"Precision@25 Mean:\", precision_25_mean)\n",
    "print(\"Precision@50 Mean:\", precision_50_mean)\n",
    "print(\"Precision@100 Mean:\", precision_100_mean)\n",
    "\n",
    "print(\"Recall@10 Mean:\", recall_10_mean)\n",
    "print(\"Recall@25 Mean:\", recall_25_mean)\n",
    "print(\"Recall@50 Mean:\", recall_50_mean)\n",
    "print(\"Recall@100 Mean:\", recall_100_mean)\n",
    "\n",
    "print(\"MAP@10 Mean:\", map_10_mean)\n",
    "print(\"MAP@25 Mean:\", map_25_mean)\n",
    "print(\"MAP@50 Mean:\", map_50_mean)\n",
    "print(\"MAP@100 Mean:\", map_100_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cf34ad-b61e-44e0-9618-79a1a8ab655b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "a_10 = precision(10)\n",
    "a_25 = precision(25)\n",
    "a_50 = precision(50)\n",
    "a_100 = precision(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfeb38b-bdc8-4cd9-878b-985bd5a8dfc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "263cd133-48ad-47ba-9813-6c0001f4911d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18898958333333335\n",
      "0.14987083333333337\n",
      "0.11610416666666667\n",
      "0.08527187500000001\n"
     ]
    }
   ],
   "source": [
    "print(np.array(a_10).mean())\n",
    "print(np.array(a_25).mean())\n",
    "print(np.array(a_50).mean())\n",
    "print(np.array(a_100).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "53d73cc9-ae8b-454e-a24f-8dae64a5a644",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm  # Import tqdm for the progress bar\n",
    "\n",
    "def Recall(k):\n",
    "    result = []\n",
    "    for i in tqdm(keys_list, desc=\"Calculating Precision\", position=0, leave=True):  # tqdm setup\n",
    "        p = []\n",
    "        try:\n",
    "            b = result_dict[i]\n",
    "            c = train_dict[i]\n",
    "        except KeyError:\n",
    "            continue\n",
    "        for j in range(num_grup):\n",
    "            p.append(list(i) + [j])  # Fix variable name from 'a' to 'i'\n",
    "        \n",
    "        p = torch.tensor(p).to(device)\n",
    "        pred = net(p).to(torch.int)  # Change to 'torch.int' for data type\n",
    "        values, sorted_indices = torch.sort(pred, descending=True)\n",
    "        \n",
    "        exclusive_indices = [idx for idx in sorted_indices.tolist() if idx not in set(c)]\n",
    "        exclusive_indices = exclusive_indices[:k]\n",
    "        \n",
    "        intersection_count = len(set(b).intersection(set(exclusive_indices)))\n",
    "        result.append(intersection_count / len(set(b)))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ff8cb724-ed67-4ebd-b506-d2accb5378e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Precision: 100%|███████████████████████████████████████████████████████| 9600/9600 [13:28<00:00, 11.87it/s]\n",
      "Calculating Precision: 100%|███████████████████████████████████████████████████████| 9600/9600 [14:53<00:00, 10.75it/s]\n",
      "Calculating Precision: 100%|███████████████████████████████████████████████████████| 9600/9600 [15:37<00:00, 10.24it/s]\n",
      "Calculating Precision: 100%|███████████████████████████████████████████████████████| 9600/9600 [15:33<00:00, 10.29it/s]\n"
     ]
    }
   ],
   "source": [
    "b_10 = Recall(10)\n",
    "b_25 = Recall(25)\n",
    "b_50 = Recall(50)\n",
    "b_100 = Recall(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "55b5e08d-c434-4bf9-973b-f0c970b27feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10634116829347541\n",
      "0.20982420153029313\n",
      "0.3227293928245148\n",
      "0.4700457197510236\n"
     ]
    }
   ],
   "source": [
    "print(np.array(b_10).mean())\n",
    "print(np.array(b_25).mean())\n",
    "print(np.array(b_50).mean())\n",
    "print(np.array(b_100).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09864743-62ae-47b8-b3ff-eb5d5328c960",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn_3.9",
   "language": "python",
   "name": "gnn_3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
